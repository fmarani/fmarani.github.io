<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Web Architectures on Blog | Federico Marani</title>
    <link>http://flagzeta.org/tags/web-architectures/</link>
    <description>Recent content in Web Architectures on Blog | Federico Marani</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</copyright>
    <lastBuildDate>Wed, 14 Jan 2015 11:45:46 +0000</lastBuildDate>
    <atom:link href="http://flagzeta.org/tags/web-architectures/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Working with caches and Memcache</title>
      <link>http://flagzeta.org/blog/working-with-memcache/</link>
      <pubDate>Wed, 14 Jan 2015 11:45:46 +0000</pubDate>
      
      <guid>http://flagzeta.org/blog/working-with-memcache/</guid>
      <description>&lt;p&gt;The traditional use of Memcache is to cache things like computations that take some time or result coming from external system with limited throughput. Examples of these may be database queries (sometimes long to compute), search queries coming from dedicated search services or simply feeds from other sites that don&amp;rsquo;t change often.&lt;/p&gt;

&lt;p&gt;Memcache is both a cache system and a separate component in your system architecture, so it is very important to understand the implications. There are many negatives: it is another dependency, which adds complexity to the system, it potentially contains stale data, and it is a potential point of failure. Please consider all these things before inserting a cache in your app, many annoying and hard to find bugs in software development are related to cache.&lt;/p&gt;

&lt;p&gt;On the positive side, if done right, caching means increase in performance for the website and less stress for the underlying service. Start by looking at the areas of your application which are executed very frequently and are computationally or IO heavy. For example, loading recursive structures (like category trees) from relational databases (which are flat) is an expensive operation.&lt;/p&gt;

&lt;p&gt;One common usecase is search: results depend only on a set of parameters which are repetitive. If parameters are not repetitive, they can be made to by approximating them to some common form: free text keywords can be stemmed (see NLP), geographic coordinates can be rounded, number of results can be made multiple of some base number, etc&amp;hellip; in this way we increase the cache hit rate without losing too much precision.&lt;/p&gt;

&lt;p&gt;This exercise of increasing cache hits is quite common, and it is also very common in compilers when they try to align structures in a way that they fit CPU caches. The principle is similar, you want to increase the likelihood of having fast read operations instead of slow operations.&lt;/p&gt;

&lt;p&gt;In python there are libraries to interact with memcache, as in Django. They are quite small as the protocol is very simple to understand.&lt;/p&gt;

&lt;p&gt;I am generally against systems that automatically cache all database queries: if you have a good schema, you should not need it. We used to cache all Postgresql queries but the speed gain was not noticeable so we removed it, we have a fast network, enough memory for the disk buffer and, honestly, not a great volume of complex queries.&lt;/p&gt;

&lt;p&gt;The setup of Memcache is really simple, you can start by just using the defaults after the installation. The default cache size is 64Mb and when exceeding that it will start to delete keys in least recently used order.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ubuntu on EC2, the simple way.</title>
      <link>http://flagzeta.org/blog/ubuntu-on-ec2-the-simple-way/</link>
      <pubDate>Sun, 06 May 2012 11:39:37 +0000</pubDate>
      
      <guid>http://flagzeta.org/blog/ubuntu-on-ec2-the-simple-way/</guid>
      <description>

&lt;h2 id=&#34;problem&#34;&gt;Problem&lt;/h2&gt;

&lt;p&gt;Sometime ago I had to run a statistical software on some data, the computation was really expensive, it was impractical to run it on my small laptop as it would hung for hours waiting for a result to come up. I thought about running it on Amazon.&lt;/p&gt;

&lt;h2 id=&#34;solution&#34;&gt;Solution&lt;/h2&gt;

&lt;p&gt;Amazon EC2 is a virtual machine hosting service, also known as IaaS. Quite similar to Linode or Rackspace. Payment here is per hour, differently from Linode&amp;hellip; slightly on the expensive side i might add, but top-end VMs are quite powerful.&lt;/p&gt;

&lt;p&gt;First step is to go through the setup procedure in order to have ec2 tools setup on your machine. I run ubuntu on my laptop and i applied the steps described &lt;a href=&#34;https://help.ubuntu.com/community/EC2StartersGuide&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After having installed the api tools and having put all EC2 environment variables in your .bashrc file, type:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ec2-describe-images -o amazon
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see the list of public AMIs from amazon. If you don&amp;rsquo;t there are problems with your configuration.&lt;/p&gt;

&lt;p&gt;By default the firewall blocks every access to every port, you have to explicitly enable access in the security group that is associated to your machine (or in the default security group).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ec2-authorize default -p 22&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This enables the ssh port. Next thing is to create the machine, i used ubuntu 11.10 64bit EBS-backed. It&amp;rsquo;s ami code is ami-895069fd. It is possible to bootstrap this specific image with a bootstrap script:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ec2-run-instances ami-895069fd -t m1.large &amp;ndash;user-data-file ~/ec2/bootstrap.sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This is an example bootstrap file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#!/bin/bash

set -e -x
export DEBIAN_FRONTEND=noninteractive
apt-get update &amp;amp;&amp;amp; apt-get upgrade -y

apt-get install -y xorg
apt-get install -y fluxbox
apt-get install -y vnc4server

wget --user=&amp;quot;YOURUSER&amp;quot; --password=&amp;quot;YOURPASS&amp;quot; -O /tmp/vnc-conf.tgz https://server/vnc-bootstrap.tgz
cd /home/ubuntu &amp;amp;&amp;amp; tar xfvz /tmp/vnc-conf.tgz &amp;amp;&amp;amp; chmod -R 700 .vnc

chmod 755 /etc/X11/xinit/xinitrc
su -c vnc4server ubuntu
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this script, i install all the packages i need and i download some initial data. For anything more serious than this i advise you to look into Puppet or Chef.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How small websites become big</title>
      <link>http://flagzeta.org/blog/how-small-websites-become-big/</link>
      <pubDate>Wed, 30 Mar 2011 00:42:56 +0000</pubDate>
      
      <guid>http://flagzeta.org/blog/how-small-websites-become-big/</guid>
      <description>&lt;p&gt;There is no secret recipe, there is no list of check boxes to tick&amp;hellip; just some guidelines. Part of these lessons have been learned in the hard way, part because i have been always taught that if you want to be the best, you have to copy the best. There is plenty of literature on Internet about this&amp;hellip; read, understand and copy.&lt;/p&gt;

&lt;p&gt;I think the art of building high-traffic websites is part about the code, but mostly about your web architecture and the tools you use. Here some points, from basic to advanced.
&lt;ul&gt;
    &lt;li&gt;Separation between web server and database server is a basic step. Do it if not for speed, for safety. If the database server gets overloaded, your web server will still be up and running. If one of the two breaks, you need half the time to go back online. If the website goes much slower, you over-rely on the database. You may have to rethink how you use databases.&lt;/li&gt;
    &lt;li&gt;Databases often are the main bottleneck of your website. Everything that is I/O related is a bottleneck because no matter what server you have, disks will always be a order of magnitude slower than memory. Consider running these servers on physical machines rather than virtual servers, with properly fast hardware.&lt;/li&gt;
    &lt;li&gt;Optimize your queries, use indexes on fields that you search on frequently. This can make a big difference. Databases are weird creatures, you need to know them well before feeling safe.&lt;/li&gt;
    &lt;li&gt;Caches are both a blessing and a curse. Using systems like Memcache (or Redis) really makes a difference. Install memcache on every webserver machine and cache all the SELECTs that can be re-used in the next X minutes. When the cache is empty, execute the query on the database and put the results in the cache for later retrieval.&lt;/li&gt;
    &lt;li&gt;Optimization makes much sense in certain areas of code. Use profiling tools to see which functions/classes get executed more often and modify that code to make it fast.&lt;/li&gt;
    &lt;li&gt;Do not blindly believe ORM is always a good solution. In fact, for heavy db tasks, do not use them.&lt;/li&gt;
    &lt;li&gt;Move all your static files on a static web server and serve them from there instead of the main web server. You will split the load without having to do any complex configuration change, other than changing base href in the html. If you have many many files, you may want to tweak the filesystem for it.&lt;/li&gt;
    &lt;li&gt;For static files, use a lightweight asynchronous web server like Nginx. Especially if you send emails with lots of images&amp;hellip; people tend to open emails as soon as they get to work or during lunch time therefore you will get very high peaks of traffic during those hours. Asynchronous web servers handle traffic spikes much better than traditional web servers.&lt;/li&gt;
    &lt;li&gt;Start adding web servers. If you use sessions, you need to store those in a space shared between all web servers, which could be database or shared drive. Shared drive is generally a good idea, put your application on it.. when you upgrade you need to do it only in one location.&lt;/li&gt;
    &lt;li&gt;Start thinking about reverse proxy, load balancers and HTTPS accelerators. Here presented in order of cost.. Reverse proxy solve the so-called &amp;ldquo;spoon feeding&amp;rdquo; problem quite well, plus you can serve cached responses if configured properly. Nginx is my favourite, followed by Varnish for complex caching policies.&lt;/li&gt;
    &lt;li&gt;Database servers are not &amp;ldquo;full text&amp;rdquo; search servers. Search is an expensive operation, must be done on dedicated systems, especially if website users do it frequently.&lt;/li&gt;
    &lt;li&gt;If you have much off-line data processing to do, do it on a dedicate server. You may want to look into Hadoop if volumes of data are enormous.&lt;/li&gt;
    &lt;li&gt;The more code and servers you have, the more likely is that something wrong happens. Learn to log events properly, with all the information you may need. In areas in which performance is really important, you may want to consider conditional logging. It is always better to have some logging than to have extremely fast code which is not debuggable when it fails.&lt;/li&gt;
    &lt;li&gt;Automatize! Having a script for everything is important. Deployment is one of the first things to automatize, especially when more than one server is involved.&lt;/li&gt;
&lt;/ul&gt;
I think i mentioned a lot of things. There are many more to mention but it is more about the management of code and project itself. Maybe in another post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CDN Optimizations</title>
      <link>http://flagzeta.org/blog/cdn-optimizations/</link>
      <pubDate>Sun, 17 Oct 2010 13:55:49 +0000</pubDate>
      
      <guid>http://flagzeta.org/blog/cdn-optimizations/</guid>
      <description>&lt;p&gt;One of our most trafficked website is on average sustaining 300000 page views per day. Each page has normally a considerable amount of JavaScript, some of it activated only after the whole DOM has been loaded.&lt;/p&gt;

&lt;p&gt;Considering that every page has on average 20-30 images coming from our image server, every small optimization to it has an avalanche effect on all the other parts of the system.&lt;/p&gt;

&lt;p&gt;I already described the infrastructure in the Nginx post. What i changed from that configuration is all I/O related, trying to minimize writes on the disk and internal connections to Apache, but the biggest change is the duration of nginx cache which is now 6 hours instead of one hour. The impact on the main site has been remarkable.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://flagzeta.org/attachments/nginx-1-to-6h-caching.jpg&#34; alt=&#34;nginx graph&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Nginx rocks</title>
      <link>http://flagzeta.org/blog/nginx-rocks/</link>
      <pubDate>Sun, 03 Oct 2010 16:09:47 +0000</pubDate>
      
      <guid>http://flagzeta.org/blog/nginx-rocks/</guid>
      <description>&lt;p&gt;I have installed Nginx some time ago on one of our busiest servers on our partner&amp;rsquo;s networks, i promised i would have blogged about this and now it&amp;rsquo;s about time. This server only serves images of products sold thorugh our e-commerce site. The first configuration was only a simple web server which was serving already resized images directly, due to the massive amount of images and the cleaning of it which was taking days, i decided to use Nginx in reverse proxy mode. I have to say i am still impressed, after 6-7 months, about this software. I&amp;rsquo;ve never had to restart it one time except for little configuration tweaks.&lt;/p&gt;

&lt;p&gt;This software is so good that we decided to put images that compose emails on this server as well. This is kind of critical because, after the nightly mail-out, there are 5-6 hours in the morning with constants spikes of traffic. Again, absolutely no side-effects, Nginx relentlessly serves gigabytes and gigabytes of images as if nothing happened.&lt;/p&gt;

&lt;p&gt;This is our configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# FIRST TIER TO ANSWER HTTP REQUESTS FOR IMAGES
# SECOND TIER IS APACHE

user  apache;
worker_processes  4; // same as number of cpu cores

error_log  logs/error.log;
pid        logs/nginx.pid;

events {
worker_connections  512;
}

http {
include       mime.types;
default_type  application/octet-stream;

log_format  main  &#39;$remote_addr - $remote_user [$time_local] &amp;quot;$request&amp;quot; &#39;
&#39;$status $body_bytes_sent &amp;quot;$http_referer&amp;quot; &#39;
&#39;&amp;quot;$http_user_agent&amp;quot; &amp;quot;$http_x_forwarded_for&amp;quot;&#39;;

# access_log  logs/access.log  main;
access_log off;

sendfile        on;
keepalive_timeout  65;

upstream imageserver {
server 127.0.0.1:81 fail_timeout=120s;
}

proxy_cache_path /opt/nginx/cache levels=2:2:2 keys_zone=imagecache:10m;
proxy_temp_path /opt/nginx/cache_temp;

server {
listen       80;
server_name  NAME.DOMAIN.COM;
#access_log  logs/host.access.log  main; //disabled for speed

root /home/website/root;
index index.php index.html index.htm;

gzip on;
gzip_disable &amp;quot;msie6&amp;quot;;
gzip_disable &amp;quot;Lynx&amp;quot;;
gzip_comp_level 8;
gzip_min_length 1000;
gzip_proxied any;
gzip_types text/plain text/css application/x-javascript text/xml application/xml application/xml+rss text/javascript;

location /nginx_status {
stub_status on;
access_log   off;
allow 127.0.0.1;
deny all;
}

location /images {
expires 15d;

client_max_body_size 8m;

proxy_redirect off;
proxy_set_header Host $host;
proxy_set_header X-Real-IP $remote_addr;
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
proxy_pass_header Set-Cookie;

proxy_cache imagecache;
proxy_cache_valid 200 302 60m;
proxy_cache_valid 404 5m;

proxy_cache_use_stale timeout;
proxy_connect_timeout 40;
proxy_read_timeout 80;

# REGEX to filter out bad image urls
if ($uri ~* &amp;quot;.*/([a-z]+)/[a-z0-9\-]+/([a-z0-9\-]+)/([0-9]+)/([0-9]+)/([a-z0-9\-]*)/([a-z0-9\-]+)\.jpg$&amp;quot;) {     #case insensitive
proxy_pass http://imageserver;
}

}

error_page   500 502 503 504  /50x.html;
location = /50x.html {
root   html;
}

location ~ /\.ht {
deny  all;
}
}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This configuration is able to serve a constant flux of 1.5Mb/s, which i reckon is about 200 requests/sec for an average file of 3Kb. Most of the requests are served directly by Nginx, some others go through to Apache which has in average 200 connnections opened.&lt;/p&gt;

&lt;p&gt;The version of nginx used is 0.7.65 compiled from sources. This because new 0.7 has an improved reverse caching module which was needed for this.&lt;/p&gt;

&lt;p&gt;EDIT: I&amp;rsquo;ve been doing some statistics on a recent normal day: 100Gb of traffic and 25-30 million images transferred. Server load was below the limit and most of it coming from Apache. Not bad at all!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>