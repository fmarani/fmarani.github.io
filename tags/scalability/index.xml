<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scalability on Blog | Federico Marani</title>
    <link>/tags/scalability/</link>
    <description>Recent content in Scalability on Blog | Federico Marani</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</copyright>
    <lastBuildDate>Sun, 25 Nov 2012 17:30:21 +0000</lastBuildDate>
    <atom:link href="/tags/scalability/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Technology behind tools.seogadget.co.uk</title>
      <link>/blog/technology-behind-toolsseogadgetcouk/</link>
      <pubDate>Sun, 25 Nov 2012 17:30:21 +0000</pubDate>
      
      <guid>/blog/technology-behind-toolsseogadgetcouk/</guid>
      <description>&lt;p&gt;Scalability was one of the primary concerns when we started building the tool. Essentially, the tool gathers numbers about links you post, it is quite straightforward. To gather these numbers, our tool uses many external APIs and in a way acts as a sort of proxy between the user and many other 3rd party API providers, on top of which some internal indicators are derived. Many tools allow you to do that, but, regarding scalability, some ways are better than others. Much better actually. Gathering information for 1000 urls a day is different than doing it on 1 million, lots of challenges came in the way.
&lt;h2&gt;TECHNOLOGY&lt;/h2&gt;
Deciding on which platform to use, we ended up using the well-known combo Python-Django-Celery. It is the one i have most experience with, and the task is really I/O bound therefore it is not one of those cases in which writing everything in C makes a big difference. This combo also allows us to code things pretty quickly, testing various methods and combinations. The real complexity is in the Celery backend, which is where the data gathering takes place.
&lt;h2&gt;WORKFLOW&lt;/h2&gt;
Requests could come in through API or through the Web interface. Web interface is a better example because that is the only way now to send multiple urls at once. When URLs enter into the system, each one of those is done in parallel. For every url, there are two rounds of data gathering, the first gets part of the final results, and then a second round gets the results that are dependent on the first round of numbers.&lt;/p&gt;

&lt;p&gt;All these single rounds of API calls are done asynchronously, not sequentially. We make heavy use of Celery advanced features such as tasksets and chords to make sure we squeeze every bit of performance we can from the system.&lt;/p&gt;

&lt;p&gt;Each background task takes then care of storing these numbers in a PostgreSql database server, which they later get pulled back in the Web interface (or API results)
&lt;h2&gt;INFRASTRUCTURE&lt;/h2&gt;
Heroku has allowed us to build something quickly, although we had to switch to an hybrid EC2 - Heroku, mainly because of heavy use of RabbitMQ. The advantage of Heroku is that you can scale the number of instances pretty quickly if there is a lot of traffic. We distribute the background tasks using RabbitMQ which has gone through some configuration changes. Some of the more interesting tweaks have gone into the configuration of Celery, especially on setting expiration limits for every single external API call to 3rd party systems. We do not want 3rd party APIs failure to bring down our service. All this has been wrapped up in a quite minimal interface, using Twitter Bootstrap as a CSS framework. Very easy to use.
&lt;h2&gt;IDEAS FOR THE FUTURE&lt;/h2&gt;
There has been some thought about improving the &amp;ldquo;spam&amp;rdquo; flag with something which can learn and adapt to new types of spam. What features to take into account when deciding about spammy links is also under review. There is also a lot of enhancements we can do on the APIs, such as different tiers, perhaps a tier with a different priority (e.g. reduced response time) or different limits which will be a paid option. There is also always room for speed improvements such as bulk queries, result caching, etc&amp;hellip; what is the feature you would like to see in this tool?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How small websites become big</title>
      <link>/blog/how-small-websites-become-big/</link>
      <pubDate>Wed, 30 Mar 2011 00:42:56 +0000</pubDate>
      
      <guid>/blog/how-small-websites-become-big/</guid>
      <description>&lt;p&gt;There is no secret recipe, there is no list of check boxes to tick&amp;hellip; just some guidelines. Part of these lessons have been learned in the hard way, part because i have been always taught that if you want to be the best, you have to copy the best. There is plenty of literature on Internet about this&amp;hellip; read, understand and copy.&lt;/p&gt;

&lt;p&gt;I think the art of building high-traffic websites is part about the code, but mostly about your web architecture and the tools you use. Here some points, from basic to advanced.
&lt;ul&gt;
    &lt;li&gt;Separation between web server and database server is a basic step. Do it if not for speed, for safety. If the database server gets overloaded, your web server will still be up and running. If one of the two breaks, you need half the time to go back online. If the website goes much slower, you over-rely on the database. You may have to rethink how you use databases.&lt;/li&gt;
    &lt;li&gt;Databases often are the main bottleneck of your website. Everything that is I/O related is a bottleneck because no matter what server you have, disks will always be a order of magnitude slower than memory. Consider running these servers on physical machines rather than virtual servers, with properly fast hardware.&lt;/li&gt;
    &lt;li&gt;Optimize your queries, use indexes on fields that you search on frequently. This can make a big difference. Databases are weird creatures, you need to know them well before feeling safe.&lt;/li&gt;
    &lt;li&gt;Caches are both a blessing and a curse. Using systems like Memcache (or Redis) really makes a difference. Install memcache on every webserver machine and cache all the SELECTs that can be re-used in the next X minutes. When the cache is empty, execute the query on the database and put the results in the cache for later retrieval.&lt;/li&gt;
    &lt;li&gt;Optimization makes much sense in certain areas of code. Use profiling tools to see which functions/classes get executed more often and modify that code to make it fast.&lt;/li&gt;
    &lt;li&gt;Do not blindly believe ORM is always a good solution. In fact, for heavy db tasks, do not use them.&lt;/li&gt;
    &lt;li&gt;Move all your static files on a static web server and serve them from there instead of the main web server. You will split the load without having to do any complex configuration change, other than changing base href in the html. If you have many many files, you may want to tweak the filesystem for it.&lt;/li&gt;
    &lt;li&gt;For static files, use a lightweight asynchronous web server like Nginx. Especially if you send emails with lots of images&amp;hellip; people tend to open emails as soon as they get to work or during lunch time therefore you will get very high peaks of traffic during those hours. Asynchronous web servers handle traffic spikes much better than traditional web servers.&lt;/li&gt;
    &lt;li&gt;Start adding web servers. If you use sessions, you need to store those in a space shared between all web servers, which could be database or shared drive. Shared drive is generally a good idea, put your application on it.. when you upgrade you need to do it only in one location.&lt;/li&gt;
    &lt;li&gt;Start thinking about reverse proxy, load balancers and HTTPS accelerators. Here presented in order of cost.. Reverse proxy solve the so-called &amp;ldquo;spoon feeding&amp;rdquo; problem quite well, plus you can serve cached responses if configured properly. Nginx is my favourite, followed by Varnish for complex caching policies.&lt;/li&gt;
    &lt;li&gt;Database servers are not &amp;ldquo;full text&amp;rdquo; search servers. Search is an expensive operation, must be done on dedicated systems, especially if website users do it frequently.&lt;/li&gt;
    &lt;li&gt;If you have much off-line data processing to do, do it on a dedicate server. You may want to look into Hadoop if volumes of data are enormous.&lt;/li&gt;
    &lt;li&gt;The more code and servers you have, the more likely is that something wrong happens. Learn to log events properly, with all the information you may need. In areas in which performance is really important, you may want to consider conditional logging. It is always better to have some logging than to have extremely fast code which is not debuggable when it fails.&lt;/li&gt;
    &lt;li&gt;Automatize! Having a script for everything is important. Deployment is one of the first things to automatize, especially when more than one server is involved.&lt;/li&gt;
&lt;/ul&gt;
I think i mentioned a lot of things. There are many more to mention but it is more about the management of code and project itself. Maybe in another post.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>