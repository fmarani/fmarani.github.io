<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Blog | Federico Marani</title>
    <link>/tags/python/</link>
    <description>Recent content in Python on Blog | Federico Marani</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</copyright>
    <lastBuildDate>Sat, 24 Sep 2016 23:16:56 +0200</lastBuildDate>
    <atom:link href="/tags/python/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>A Python microservice stack</title>
      <link>/blog/a-python-microservice-stack/</link>
      <pubDate>Sat, 24 Sep 2016 23:16:56 +0200</pubDate>
      
      <guid>/blog/a-python-microservice-stack/</guid>
      <description>

&lt;p&gt;First of all, let me say the word &amp;ldquo;microservice&amp;rdquo; is incredibly inflated these days, but some of the original reasons to use them still hold true. Part of its definition comes from SOA, with some added considerations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Services roughly map to business functions&lt;/li&gt;
&lt;li&gt;Services are autonomous (shared nothing architecture)&lt;/li&gt;
&lt;li&gt;Boundaries are explicit&lt;/li&gt;
&lt;li&gt;Services declare schemas and interfaces&lt;/li&gt;
&lt;li&gt;Company policy defines version compatibility&lt;/li&gt;
&lt;li&gt;Services are deployed separately&lt;/li&gt;
&lt;li&gt;Services are manageable by different teams&lt;/li&gt;
&lt;li&gt;Service unavailability is handled gracefully&lt;/li&gt;
&lt;li&gt;Service call stack is broad rather than deep&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Recently I worked with a company that needed to scale product/engineering to 50+ people, which makes the investment towards this methodology justifiable. There is a cost in managing all this, but people&amp;rsquo;s autonomy pays this off quickly. In the future this cost might change, but I doubt it is going to be zero. Consider that it is much harder to change a microservice ecosystem than to change the layers of a monolithic software, because of its distributed nature.&lt;/p&gt;

&lt;p&gt;Regarding the tools for this, I put together a proof of concept of a microservice stack. It is focused on Python to start with, but I picked tools that can be used from any language. In these environments you also do not need a generic framework like Django anymore, you can pick something more lightweight if the current service allows it.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GRPC&lt;/li&gt;
&lt;li&gt;Consul&lt;/li&gt;
&lt;li&gt;StatsD&lt;/li&gt;
&lt;li&gt;Kafka&lt;/li&gt;
&lt;li&gt;SQLAlchemy/Alembic (Python specific)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All these tools together allowed me to put together a simplified version of a service they had in production already.&lt;/p&gt;

&lt;p&gt;GRPC is a modern RPC library based on HTTP/2 and Protocol Buffers. The reason why I picked this instead of REST or a messaging queue is that it makes schemas and interfaces explicit. One could argue that Swagger (or similar) can be used for the same purpose, but in reality I have seen these files get out of date quickly because people forget.&lt;/p&gt;

&lt;p&gt;GPRC forces you to generate client/server code out of &amp;ldquo;documentation&amp;rdquo; files, which makes them always be up to date. It is also an RPC protocol, which is inherently more flexible than a REST system. In addition to this, HTTP/2 is a multiplexed protocol, which finally means we are not forced to fit everything in one response, just because the frontend team wants performance.&lt;/p&gt;

&lt;p&gt;Consul allows you to do mainly 2 things, service discovery and configuration storage. Service discovery is useful in case you want to stop thinking about servers as reference point for your architecture: your service should not rely on always be on the same server(s). Location may change in case servers get rebuilt and IPs change. This tool gives you a lot more flexibility in modifying your server infrastructure. Consul has also integrated healthcheck, which works with GRPC, so in case servers go down, you are redirected to a working copy on a different server.&lt;/p&gt;

&lt;p&gt;Consul configuration storage is something that you may not decide to use it if you already have a highly available database, but what makes Consul stand out is the ability to watch for changes to selected keys, and trigger microservice restarts. There is a nice utility built by Hashicorp called envconsul, which copies values from Consul to environment variables, and restarts services, which is for instance the behaviour of Heroku.&lt;/p&gt;

&lt;p&gt;Logging and monitoring are critical in distributed environments. StatsD and Kafka are both suitable for high-traffic environments, due to being battle-tested tech, and having asynchronous client libraries helps with this. Even though in theory you could base your monitoring on your logging system, I found that when you have high-throughput logging, the system tends to drop logging packets, and you do not get the full numbers. That is why I like separation between logging and monitoring.&lt;/p&gt;

&lt;p&gt;Kafka seems to be all the craze these days. It is a messaging queue system unlike any other. It is not based on ack-ing messages (queues) or simple pub-sub (topics) but on a form of pub-sub with the ability to go back in time by specifying different starting offsets. It is an ideal setting for many applications, such as logging data processing. That is because it saves complexity server-side (explicit message marking) and client bandwidth (ack transmission), with the cost of mantaining more state on the client (offsets) and potentially a big storage buffer on the server.&lt;/p&gt;

&lt;p&gt;Kafka allows you to process all messages, have at-least-once or at-most-once delivery, and a performant system, given that you are able to recover from client crash pretty quickly. If the time to recover is within the retention period of your Kafka queues, you will not have lost any message.&lt;/p&gt;

&lt;h2 id=&#34;the-code&#34;&gt;The code&lt;/h2&gt;

&lt;p&gt;Here is a very simple client/server implementation of a search service by geometry and/or name.&lt;/p&gt;

&lt;h3 id=&#34;proto-file&#34;&gt;Proto file&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;syntax = &amp;quot;proto3&amp;quot;;

import &amp;quot;google/protobuf/empty.proto&amp;quot;;

message SearchRequest {
  string query = 1;
  float lat = 4;
  float lng = 5;
  int32 page_number = 2;
  int32 result_per_page = 3;
}

message SearchResponses {
    repeated SearchResponse responses = 1;
}

message SearchResponse {
    string response = 1;
}

message MonitorResponse {
    int32 n_things = 1;
}

service Search {
    rpc monitor(google.protobuf.Empty) returns (MonitorResponse) {}
    rpc search(SearchRequest) returns (SearchResponses) {}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can then generate the server code and stub client code with this command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;python -m grpc.tools.protoc -I. --python_out=. --grpc_python_out=. search.proto
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point you should get a file you can import in both client and server code. The client code is almost ready to use, while the server code gives you an abstract server class with empty methods that you would need to implement. In my version of GRPC, the file is called &lt;code&gt;search_pb2.py&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;server&#34;&gt;Server&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;import time
import sys
import consul
import logging
import statsd
import random
import os

import search_pb2
from models import session, Thing, func

log = logging.getLogger()
log.setLevel(logging.DEBUG)

ch = logging.StreamHandler(sys.stdout)
ch.setLevel(logging.DEBUG)
formatter = logging.Formatter(&#39;%(asctime)s - %(name)s - %(levelname)s - %(message)s&#39;)
ch.setFormatter(formatter)
log.addHandler(ch)


_ONE_DAY_IN_SECONDS = 60 * 60 * 24

port = random.randint(50000, 59000)
stat = statsd.StatsClient(&#39;localhost&#39;, 8125)

class SearchServicer(search_pb2.SearchServicer):
    @stat.timer(&amp;quot;search&amp;quot;)
    def search(self, request, context):
        stat.incr(&amp;quot;search_count&amp;quot;)
        log.info(&amp;quot;search request: &amp;quot; + str(request))
        query = session.query(Thing).filter(
                func.ST_Contains(Thing.geom, &#39;POINT({} {})&#39;.format(request.lat, request.lng)))
        responses = [search_pb2.SearchResponse(response=rec.name) for rec in query]
        log.info(&amp;quot;search responses: &amp;quot; + str(responses))
        return search_pb2.SearchResponses(responses=responses)

    @stat.timer(&amp;quot;monitor&amp;quot;)
    def monitor(self, request, context):
        stat.incr(&amp;quot;monitor_count&amp;quot;)
        n_things = session.query(Thing).count()
        return search_pb2.MonitorResponse(n_things=n_things)

def register():
    log.info(&amp;quot;register started&amp;quot;)
    c = consul.Consul()
    check = consul.Check.tcp(&amp;quot;127.0.0.1&amp;quot;, port, &amp;quot;30s&amp;quot;)
    c.agent.service.register(&amp;quot;search-service&amp;quot;, &amp;quot;search-service-%d&amp;quot; % port, address=&amp;quot;127.0.0.1&amp;quot;, port=port, check=check)
    log.info(&amp;quot;services: &amp;quot; + str(c.agent.services()))

def unregister():
    log.info(&amp;quot;unregister started&amp;quot;)
    c = consul.Consul()
    c.agent.service.deregister(&amp;quot;search-service-%d&amp;quot; % port)
    log.info(&amp;quot;services: &amp;quot; + str(c.agent.services()))

def serve():
  server = search_pb2.beta_create_Search_server(SearchServicer())
  server.add_insecure_port(&#39;[::]:&#39; + str(port))
  server.start()
  log.info(&amp;quot;server started&amp;quot;)
  try:
    while True:
      time.sleep(_ONE_DAY_IN_SECONDS)
  except KeyboardInterrupt:
    server.stop(0)


if __name__ == &#39;__main__&#39;:
    register()
    serve()
    unregister()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Along with the required models:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from sqlalchemy import create_engine, func
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy import Column, Integer, String
from geoalchemy2 import Geometry
from sqlalchemy.orm import sessionmaker


engine = create_engine(&amp;quot;postgresql://user:password@localhost:5434/test&amp;quot;)
Base = declarative_base()

class Thing(Base):
    __tablename__ = &amp;quot;thing&amp;quot;
    id = Column(Integer, primary_key=True)
    name = Column(String)
    geom = Column(Geometry(&#39;POLYGON&#39;))

Session = sessionmaker(bind=engine)
session = Session()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this very simple API server, you can see GRPC, Consul, statsD and SqlAlchemy all blended together, as a proof of concept. The server responds to 2 functions, one is the search function and another called monitor that returns some internal stats around the service.&lt;/p&gt;

&lt;p&gt;Once you populated Postgresql with some data, you should be able to query the service with the client.&lt;/p&gt;

&lt;h3 id=&#34;client&#34;&gt;Client&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;import grpc
import sys
import logging
from dns import resolver

import search_pb2

log = logging.getLogger()
log.setLevel(logging.DEBUG)

ch = logging.StreamHandler(sys.stdout)
ch.setLevel(logging.DEBUG)
formatter = logging.Formatter(&#39;%(asctime)s - %(name)s - %(levelname)s - %(message)s&#39;)
ch.setFormatter(formatter)
log.addHandler(ch)

consul_resolver = resolver.Resolver()
consul_resolver.port = 8600
consul_resolver.nameservers = [&amp;quot;127.0.0.1&amp;quot;]

dnsanswer = consul_resolver.query(&amp;quot;search-service.service.consul&amp;quot;, &#39;A&#39;)
ip = str(dnsanswer[0])
dnsanswer_srv = consul_resolver.query(&amp;quot;search-service.service.consul&amp;quot;, &#39;SRV&#39;)
port = int(str(dnsanswer_srv[0]).split()[2])

log.info(&amp;quot;creating grpc client based on consul data: ip=%s port=%d&amp;quot; % (ip, port))
channel = grpc.insecure_channel(&#39;%s:%d&#39; % (ip, port))
stub = search_pb2.SearchStub(channel)

if len(sys.argv) == 1 and sys.argv[1] == &amp;quot;--monitor&amp;quot;:
    monitresp = stub.monitor(search_pb2.google_dot_protobuf_dot_empty__pb2.Empty())
    log.debug(&amp;quot;monitor response: {}&amp;quot;.format(monitresp))
else:
    req = search_pb2.SearchRequest(
        query=&amp;quot;queryabc&amp;quot;,
        lat=float(sys.argv[1]),
        lng=float(sys.argv[2]),
        result_per_page=10)
    log.debug(&amp;quot;sending request: {}&amp;quot;.format(req))
    resp = stub.search(req)
    log.debug(&amp;quot;received response: {}&amp;quot;.format(resp))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The client is querying Consul DNS service to find the microservice, using the dnspython library.&lt;/p&gt;

&lt;h2 id=&#34;underlying-services&#34;&gt;Underlying services&lt;/h2&gt;

&lt;h3 id=&#34;consul&#34;&gt;Consul&lt;/h3&gt;

&lt;p&gt;As the code is written above, Consul is not optional. In order to quickly test the setup above, you can use Docker. Please consider that the following command does not start Consul with failover or any option that you would want to use in production.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run --net=host consul agent -server -ui -bootstrap -bind=127.0.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With Consul, you are free to have multiple copies of the API server running on multiple nodes, and requests will be resolved with Round Robin. The healthcheck will take a copy of the service off the DNS list if it becomes unresponsive. You can also stop the service and before exiting, it will deregister itself.&lt;/p&gt;

&lt;p&gt;In order to get the benefits of centralized configuration management, you can start the service with envconsul:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./envconsul -consul localhost:8500 -prefix search-service python search_server.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can then add variables through the Consul UI and the service will be restarted automatically.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../attachments/consulkv.png&#34; alt=&#34;consul&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;statsd&#34;&gt;StatsD&lt;/h3&gt;

&lt;p&gt;Again, you can use Docker to get running pretty quickly. Let&amp;rsquo;s start Graphite on port 8002 and StatsD on 8125.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -p 8002:80 -p 8125:8125/udp -d samsaffron/graphite
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The use we do here is quite basic, but it is enough to get an idea of the load on the service. With this data you can do estimates whether you have to run this on additional servers, or do some additional optimizations to the code.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../attachments/graphite.png&#34; alt=&#34;graphite&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;kafka&#34;&gt;Kafka&lt;/h3&gt;

&lt;p&gt;This component is optional in the above setup. If you run the code as it is, all logging goes to stdout. I find this very valuable when you are developing on your local machine. But on staging/production environments, you may want to stream the logs, and that is when you pick a tool like Kafka.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../attachments/grpc-server.png&#34; alt=&#34;grpc-server&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start a copy of Kafka by using the Docker image created by Spotify&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run -p 2181:2181 -p 9092:9092 --env ADVERTISED_HOST=127.0.0.1 --env ADVERTISED_PORT=9092 spotify/kafka
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After Kafka has succcessfully started, you can modify the envconsul command above to pipe all output to Kafka.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./envconsul -consul localhost:8500 -prefix search-service python search_server.py | kafkacat -P -b localhost -t search-service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And see all output back using kafkacat again:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafkacat -b localhost -t search-service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This use of Kafka is quite basic, but it offers already enough to do a distributed &amp;ldquo;tail -f&amp;rdquo; of a service, regardless of its location. I will blog a bit more about a more advanced use of Kafka, for now that is all.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spatial search on multiple points in Solr</title>
      <link>/blog/spatial-search-on-multiple-points-in-solr/</link>
      <pubDate>Wed, 31 Jul 2013 12:09:43 +0000</pubDate>
      
      <guid>/blog/spatial-search-on-multiple-points-in-solr/</guid>
      <description>&lt;p&gt;At TrialReach we deal with clinical trials data, which contain a lot of spatial information. Tipically, clinical trials treat a certain set of conditions and they happen in various locations globally.
If you are a patient then searching across clinical trials becomes really spatial sensitive: you are only interested in the closest location to you.&lt;/p&gt;

&lt;p&gt;This case might apply to other events as well, but the key point is global distribution. I am not interested in any point in the globe, just the closest to me.&lt;/p&gt;

&lt;p&gt;&lt;h2&gt;Solution&lt;/h2&gt;
Solr 4 does have support for this with the new spatial field called SpatialRecursivePrefixTreeFieldType, with many caveats though.&lt;/p&gt;

&lt;p&gt;A schema could look this way:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot; ?&amp;gt;
&amp;lt;schema name=&amp;quot;example&amp;quot; version=&amp;quot;1.5&amp;quot;&amp;gt;
 &amp;lt;fields&amp;gt;
   &amp;lt;field name=&amp;quot;id&amp;quot; type=&amp;quot;string&amp;quot; indexed=&amp;quot;true&amp;quot; stored=&amp;quot;true&amp;quot; required=&amp;quot;true&amp;quot; multiValued=&amp;quot;false&amp;quot; /&amp;gt; 
   &amp;lt;field name=&amp;quot;title&amp;quot; type=&amp;quot;text_en&amp;quot; indexed=&amp;quot;true&amp;quot; stored=&amp;quot;true&amp;quot; required=&amp;quot;true&amp;quot;/&amp;gt;
   &amp;lt;field name=&amp;quot;condition&amp;quot; type=&amp;quot;text_en&amp;quot; indexed=&amp;quot;true&amp;quot; stored=&amp;quot;true&amp;quot; required=&amp;quot;true&amp;quot; multiValued=&amp;quot;true&amp;quot;/&amp;gt;
   &amp;lt;field name=&amp;quot;location&amp;quot; type=&amp;quot;location_rpt&amp;quot; indexed=&amp;quot;true&amp;quot; stored=&amp;quot;true&amp;quot; multiValued=&amp;quot;true&amp;quot;/&amp;gt;
   &amp;lt;field name=&amp;quot;_version_&amp;quot; type=&amp;quot;long&amp;quot; indexed=&amp;quot;true&amp;quot; stored=&amp;quot;true&amp;quot; /&amp;gt;
 &amp;lt;/fields&amp;gt;
 ... 
  &amp;lt;types&amp;gt;
 ...
    &amp;lt;fieldType name=&amp;quot;location_rpt&amp;quot; class=&amp;quot;solr.SpatialRecursivePrefixTreeFieldType&amp;quot;
        geo=&amp;quot;true&amp;quot; distErrPct=&amp;quot;0.025&amp;quot; maxDistErr=&amp;quot;0.000009&amp;quot; units=&amp;quot;degrees&amp;quot; /&amp;gt;
 &amp;lt;/types&amp;gt;
&amp;lt;/schema&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A sample indexer using GeoDjango and PySolr (Haystack does not support this). It should be quite easy to work out how it works, PySolr is just a very thin wrapper for doing HTTP POST requests to Apache Solr.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pysolr

solr = pysolr.Solr(&amp;quot;http://1.2.3.4:8983/solr/&amp;quot;, timeout=10)

records = models.Study.objects.all()
solr_data = []
for record in records:
    solr_dict = {
                &amp;quot;id&amp;quot;: str(record.id),
                &amp;quot;title&amp;quot;: record.title,
                &amp;quot;condition&amp;quot;: [c.name for c in record.conditions.all()],
                &amp;quot;location&amp;quot;: [&amp;quot;{1} {0}&amp;quot;.format(l.point.coords[0], l.point.coords[1]) for l in record.locations.all()],
		# &amp;quot;point&amp;quot; is a Point GeoDjango type
		# SOLR FORMAT is &amp;quot;long lat&amp;quot;, separated by a space
            }
    solr_data.append(solr_dict)
solr.add(solr_data)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For querying, we use these sort of urls:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://1.2.3.4:8983/solr/select/?sort=score+asc&amp;amp;fq=title:lupus+condition:lupus&amp;amp;q={!geofilt score=distance sfield=location pt=LAT,LONG d=KM_RADIUS}&amp;amp;fl=*,score
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;to return the distance you need to use the score, and the only thing you use in the q parameter is the geofilt (otherwise will influence the score), all other filters go in fq&lt;/li&gt;
&lt;li&gt;if you do not need the distance, loose the score parameter in geofilt (it is inefficient)&lt;/li&gt;
&lt;li&gt;distance returned is the distance between specified LAT,LONG and the closest LAT,LONG in the SpatialRecursivePrefixTreeFieldType set.&lt;/li&gt;
&lt;li&gt;score returned is in DEGREES. You have to convert it in Km or miles.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;h2&gt;Shortcomings&lt;/h2&gt;
- the only way to get the distance is through the score
- you cannot get the matched point through highlighting or any other way
- units of measure are a bit confusing&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Text classification in Python</title>
      <link>/blog/text-classification-in-python/</link>
      <pubDate>Wed, 21 Mar 2012 22:44:23 +0000</pubDate>
      
      <guid>/blog/text-classification-in-python/</guid>
      <description>&lt;p&gt;Python and NLTK form quite a good platform to do text analysis. There is a lot of information on Internet, nevertheless i have not found a clean and simple example of a classifier. Text classifiers come from techniques such as Natural Language Processing and Machine Learning, in fact i think they are exactly in the middle of these.&lt;/p&gt;

&lt;p&gt;Bearing in mind that building a good classifier is only possible when you have a training set that represents reality quite well, and certainly longer than the one in this example, here a first stab at it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import nltk
import itertools
import sys
import random

class Classifier(object):
    &amp;quot;&amp;quot;&amp;quot;classify by looking at a site&amp;quot;&amp;quot;&amp;quot;
    def __init__(self, training_set):
        self.training_set = training_set
        self.stopwords = nltk.corpus.stopwords.words(&amp;quot;english&amp;quot;)
        self.stemmer = nltk.PorterStemmer()
        self.minlength = 7
        self.maxlength = 25
    def text_process_entry(self, example):
        site_text = nltk.clean_html(example[0]).lower()
        original_tokens = itertools.chain.from_iterable(nltk.word_tokenize(w) for w in nltk.sent_tokenize(site_text))
        tokens = original_tokens #+ [&#39; &#39;.join(w) for w in nltk.util.ngrams(original_tokens, 2)]
        tokens = [w for w in tokens if not w in self.stopwords]
        tokens = [w for w in tokens if self.minlength &amp;lt; len(w) &amp;lt; self.maxlength]
        #tokens = [self.stemmer.stem(w) for w in tokens]
        return (tokens, example[1])
    def text_process_all(self, exampleset):
        processed_training_set = [self.text_process_entry(i) for i in self.training_set]
        processed_training_set = filter(lambda x: len(x[0]) &amp;gt; 0, processed_training_set) # remove empty crawls
        processed_texts = [i[0] for i in processed_training_set]
        all_words = nltk.FreqDist(itertools.chain.from_iterable(processed_texts))
        features_to_test = all_words.keys()[:5000]
        self.features_to_test = features_to_test
        featuresets = [(self.document_features(d), c) for (d,c) in processed_training_set]
        return featuresets
    def document_features(self, document):
        #document_words = set(document)
        features = {}
        for word in self.features_to_test:
            #features[&#39;contains(%s)&#39; % word] = (word in document_words)
            features[&#39;contains(%s)&#39; % word] = (word in document)
            #features[&#39;occurrencies(%s)&#39; % word] = document.count(word) 
            #features[&#39;atleast3(%s)&#39; % word] = document.count(word) &amp;gt; 3
        return features
    def build_classifier(self, featuresets):
        random.shuffle(featuresets)
        cut_point = len(featuresets) / 5
        train_set, test_set = featuresets[cut_point:], featuresets[:cut_point]
        classifier = nltk.NaiveBayesClassifier.train(train_set)
        return (classifier, test_set)
    def run(self):
        featuresets = self.text_process_all(self.training_set)
        classifier, test_set = self.build_classifier(featuresets)
        self.classifier = classifier
        self.test_classifier(classifier, test_set)
    def classify(self, text):
        return self.classifier.classify(self.document_features(text))
    def test_classifier(self, classifier, test_set):
        print nltk.classify.accuracy(classifier, test_set)
        classifier.show_most_informative_features(45)

classes = (&#39;a la carte&#39;, &#39;advertising&#39;, &#39;commission&#39;, &#39;investment&#39;, &#39;pay as you go&#39;)

training_set = [
    (&#39;we are a bank specialized in dealing with IT companies&#39;, classes[3]),
    (&#39;we sell our product at a fixed cost of 10 pounds&#39;, classes[0]),
    (&#39;the cost per click is 0.01 dollars but if you get more than 10000 impression the cost will be 0.12&#39;, classes[1]),
    (&#39;we take a 1% commission on all sales, overseas sales have an additional charge of 12%&#39;, classes[2]),
    (&#39;we charge a 1% on top of your final price.&#39;, classes[2]),
    (&#39;we sell our product at 5 pounds, excluding with the variant A which costs an extra of 55 pounds&#39;, classes[0]),
    (&#39;we sell our product at 6 pounds, excluding with the variant B which costs 45 pounds&#39;, classes[0]),
    (&#39;our commission is normally between 1% and 2%&#39;, classes[2]),
    (&#39;impressions on the homepage on sundays are worth 0.01 pounds&#39;, classes[1]),
    (&#39;we will show impressions only to users that correspond to certain criteria.&#39;, classes[1]),
    (&#39;we manage an hedge fund and we take care of placing investments on behalf of our clients&#39;, classes[3]),
    (&#39;we bill only for the amount of api you use. 0.10 per 1000 calls&#39;, classes[4]),
    (&#39;running a virtual machine will cost you 0.12 pounds per hour&#39;, classes[4]),
    (&#39;we invest in major hedge funds&#39;, classes[3]),
    (&#39;we are an international bank, based in all countries of europe&#39;, classes[3]),
]

test_text = &amp;quot;we are a hedge fund collaborating with many banks in europe&amp;quot;
test_text2 = &amp;quot;we charge a fixed fee on top of our client&#39;s sales&amp;quot;

if __name__ == &#39;__main__&#39;:
    classifier = Classifier(training_set)
    classifier.run()
    print &amp;quot;%s -&amp;gt; classified as: %s&amp;quot; % (test_text, classifier.classify(test_text))
    print &amp;quot;%s -&amp;gt; classified as: %s&amp;quot; % (test_text2, classifier.classify(test_text2))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can run this code and classify entities based on their preferred sales target. Some of the above lines are commented, uncomment them if you think it gives you a better representation of the example. Just add a further 1000 good examples and then it should start to make accurate decisions&amp;hellip; enjoy!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Logging</title>
      <link>/blog/logging/</link>
      <pubDate>Fri, 08 Apr 2011 01:00:10 +0000</pubDate>
      
      <guid>/blog/logging/</guid>
      <description>&lt;p&gt;I work on e-commerce platforms, logging is a critical component in this area. Learn how to do it correctly will allow you and everybody else to save time when problems happen. Sometimes it is not just about saving time, but also being able to give correct answers to customers when things go wrong. When interacting with third parties, logging is even more important, because it allows you to understand where the problem lies, in your code or in the external service.&lt;/p&gt;

&lt;p&gt;First basic rule is use a logging framework. There are plenty out there, for any language. Modern logging framework are quite complex, they allow you to use different methods of logging (file, database, etc..), log rotation, log levels, email triggering, etc&amp;hellip; you should refrain from inventing your own, usually these are pretty extensible anyway. All of the frameworks i have seen have usually the same sort of interface, level based&amp;hellip; and they are quite easy to use.&lt;/p&gt;

&lt;p&gt;Once you set up your framework, there is a lot to say about how you log things. Do not pretend that people already know your code inside out when they read your application logs. In fact, high are the chances that you do not remember how you coded the functionality if you are looking at logs of old code. Do not give implementation insights in the log files, talk about what the code is doing using the language of the application domain. If you are building a billing application, use billing specific vocabularies. If it is a payment gateway, try to reuse the 3rd party terminology. If you need to, you can quickly lookup the term you used in their documentation and discover what that means. That is something you cannot do if you made that up.&lt;/p&gt;

&lt;p&gt;After having chosen how to log things, consider every log call and give it a degree of severity. From an ignorant point of view, you know you should start worrying when you see lots of &amp;ldquo;Critical&amp;rdquo; inside the log files, even if you do not know the application domain. If you see one, it is probably a good idea to take a look at the code or at least notify the author.&lt;/p&gt;

&lt;p&gt;For critical errors or errors that need to be fixed, useful is to include stats (cpu, memory usage, etc) and location of the code that was being executed. If you had an exception triggered, log the exception stack trace.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>