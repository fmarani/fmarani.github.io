<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Blog | Federico Marani</title>
    <link>http://flagzeta.org/tags/python/</link>
    <description>Recent content in Python on Blog | Federico Marani</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</copyright>
    <lastBuildDate>Wed, 31 Jul 2013 12:09:43 +0000</lastBuildDate>
    <atom:link href="http://flagzeta.org/tags/python/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Spatial search on multiple points in Solr</title>
      <link>http://flagzeta.org/blog/spatial-search-on-multiple-points-in-solr/</link>
      <pubDate>Wed, 31 Jul 2013 12:09:43 +0000</pubDate>
      
      <guid>http://flagzeta.org/blog/spatial-search-on-multiple-points-in-solr/</guid>
      <description>&lt;p&gt;At TrialReach we deal with clinical trials data, which contain a lot of spatial information. Tipically, clinical trials treat a certain set of conditions and they happen in various locations globally.
If you are a patient then searching across clinical trials becomes really spatial sensitive: you are only interested in the closest location to you.&lt;/p&gt;

&lt;p&gt;This case might apply to other events as well, but the key point is global distribution. I am not interested in any point in the globe, just the closest to me.&lt;/p&gt;

&lt;p&gt;&lt;h2&gt;Solution&lt;/h2&gt;
Solr 4 does have support for this with the new spatial field called SpatialRecursivePrefixTreeFieldType, with many caveats though.&lt;/p&gt;

&lt;p&gt;A schema could look this way:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot; ?&amp;gt;
&amp;lt;schema name=&amp;quot;example&amp;quot; version=&amp;quot;1.5&amp;quot;&amp;gt;
 &amp;lt;fields&amp;gt;
   &amp;lt;field name=&amp;quot;id&amp;quot; type=&amp;quot;string&amp;quot; indexed=&amp;quot;true&amp;quot; stored=&amp;quot;true&amp;quot; required=&amp;quot;true&amp;quot; multiValued=&amp;quot;false&amp;quot; /&amp;gt; 
   &amp;lt;field name=&amp;quot;title&amp;quot; type=&amp;quot;text_en&amp;quot; indexed=&amp;quot;true&amp;quot; stored=&amp;quot;true&amp;quot; required=&amp;quot;true&amp;quot;/&amp;gt;
   &amp;lt;field name=&amp;quot;condition&amp;quot; type=&amp;quot;text_en&amp;quot; indexed=&amp;quot;true&amp;quot; stored=&amp;quot;true&amp;quot; required=&amp;quot;true&amp;quot; multiValued=&amp;quot;true&amp;quot;/&amp;gt;
   &amp;lt;field name=&amp;quot;location&amp;quot; type=&amp;quot;location_rpt&amp;quot; indexed=&amp;quot;true&amp;quot; stored=&amp;quot;true&amp;quot; multiValued=&amp;quot;true&amp;quot;/&amp;gt;
   &amp;lt;field name=&amp;quot;_version_&amp;quot; type=&amp;quot;long&amp;quot; indexed=&amp;quot;true&amp;quot; stored=&amp;quot;true&amp;quot; /&amp;gt;
 &amp;lt;/fields&amp;gt;
 ... 
  &amp;lt;types&amp;gt;
 ...
    &amp;lt;fieldType name=&amp;quot;location_rpt&amp;quot; class=&amp;quot;solr.SpatialRecursivePrefixTreeFieldType&amp;quot;
        geo=&amp;quot;true&amp;quot; distErrPct=&amp;quot;0.025&amp;quot; maxDistErr=&amp;quot;0.000009&amp;quot; units=&amp;quot;degrees&amp;quot; /&amp;gt;
 &amp;lt;/types&amp;gt;
&amp;lt;/schema&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A sample indexer using GeoDjango and PySolr (Haystack does not support this). It should be quite easy to work out how it works, PySolr is just a very thin wrapper for doing HTTP POST requests to Apache Solr.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pysolr

solr = pysolr.Solr(&amp;quot;http://1.2.3.4:8983/solr/&amp;quot;, timeout=10)

records = models.Study.objects.all()
solr_data = []
for record in records:
    solr_dict = {
                &amp;quot;id&amp;quot;: str(record.id),
                &amp;quot;title&amp;quot;: record.title,
                &amp;quot;condition&amp;quot;: [c.name for c in record.conditions.all()],
                &amp;quot;location&amp;quot;: [&amp;quot;{1} {0}&amp;quot;.format(l.point.coords[0], l.point.coords[1]) for l in record.locations.all()],
        # &amp;quot;point&amp;quot; is a Point GeoDjango type
        # SOLR FORMAT is &amp;quot;long lat&amp;quot;, separated by a space
            }
    solr_data.append(solr_dict)
solr.add(solr_data)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For querying, we use these sort of urls:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://1.2.3.4:8983/solr/select/?sort=score+asc&amp;amp;fq=title:lupus+condition:lupus&amp;amp;q={!geofilt score=distance sfield=location pt=LAT,LONG d=KM_RADIUS}&amp;amp;fl=*,score
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;to return the distance you need to use the score, and the only thing you use in the q parameter is the geofilt (otherwise will influence the score), all other filters go in fq&lt;/li&gt;
&lt;li&gt;if you do not need the distance, loose the score parameter in geofilt (it is inefficient)&lt;/li&gt;
&lt;li&gt;distance returned is the distance between specified LAT,LONG and the closest LAT,LONG in the SpatialRecursivePrefixTreeFieldType set.&lt;/li&gt;
&lt;li&gt;score returned is in DEGREES. You have to convert it in Km or miles.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;h2&gt;Shortcomings&lt;/h2&gt;
- the only way to get the distance is through the score
- you cannot get the matched point through highlighting or any other way
- units of measure are a bit confusing&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Text classification in Python</title>
      <link>http://flagzeta.org/blog/text-classification-in-python/</link>
      <pubDate>Wed, 21 Mar 2012 22:44:23 +0000</pubDate>
      
      <guid>http://flagzeta.org/blog/text-classification-in-python/</guid>
      <description>&lt;p&gt;Python and NLTK form quite a good platform to do text analysis. There is a lot of information on Internet, nevertheless i have not found a clean and simple example of a classifier. Text classifiers come from techniques such as Natural Language Processing and Machine Learning, in fact i think they are exactly in the middle of these.&lt;/p&gt;

&lt;p&gt;Bearing in mind that building a good classifier is only possible when you have a training set that represents reality quite well, and certainly longer than the one in this example, here a first stab at it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import nltk
import itertools
import sys
import random

class Classifier(object):
    &amp;quot;&amp;quot;&amp;quot;classify by looking at a site&amp;quot;&amp;quot;&amp;quot;
    def __init__(self, training_set):
        self.training_set = training_set
        self.stopwords = nltk.corpus.stopwords.words(&amp;quot;english&amp;quot;)
        self.stemmer = nltk.PorterStemmer()
        self.minlength = 7
        self.maxlength = 25
    def text_process_entry(self, example):
        site_text = nltk.clean_html(example[0]).lower()
        original_tokens = itertools.chain.from_iterable(nltk.word_tokenize(w) for w in nltk.sent_tokenize(site_text))
        tokens = original_tokens #+ [&#39; &#39;.join(w) for w in nltk.util.ngrams(original_tokens, 2)]
        tokens = [w for w in tokens if not w in self.stopwords]
        tokens = [w for w in tokens if self.minlength &amp;lt; len(w) &amp;lt; self.maxlength]
        #tokens = [self.stemmer.stem(w) for w in tokens]
        return (tokens, example[1])
    def text_process_all(self, exampleset):
        processed_training_set = [self.text_process_entry(i) for i in self.training_set]
        processed_training_set = filter(lambda x: len(x[0]) &amp;gt; 0, processed_training_set) # remove empty crawls
        processed_texts = [i[0] for i in processed_training_set]
        all_words = nltk.FreqDist(itertools.chain.from_iterable(processed_texts))
        features_to_test = all_words.keys()[:5000]
        self.features_to_test = features_to_test
        featuresets = [(self.document_features(d), c) for (d,c) in processed_training_set]
        return featuresets
    def document_features(self, document):
        #document_words = set(document)
        features = {}
        for word in self.features_to_test:
            #features[&#39;contains(%s)&#39; % word] = (word in document_words)
            features[&#39;contains(%s)&#39; % word] = (word in document)
            #features[&#39;occurrencies(%s)&#39; % word] = document.count(word) 
            #features[&#39;atleast3(%s)&#39; % word] = document.count(word) &amp;gt; 3
        return features
    def build_classifier(self, featuresets):
        random.shuffle(featuresets)
        cut_point = len(featuresets) / 5
        train_set, test_set = featuresets[cut_point:], featuresets[:cut_point]
        classifier = nltk.NaiveBayesClassifier.train(train_set)
        return (classifier, test_set)
    def run(self):
        featuresets = self.text_process_all(self.training_set)
        classifier, test_set = self.build_classifier(featuresets)
        self.classifier = classifier
        self.test_classifier(classifier, test_set)
    def classify(self, text):
        return self.classifier.classify(self.document_features(text))
    def test_classifier(self, classifier, test_set):
        print nltk.classify.accuracy(classifier, test_set)
        classifier.show_most_informative_features(45)

classes = (&#39;a la carte&#39;, &#39;advertising&#39;, &#39;commission&#39;, &#39;investment&#39;, &#39;pay as you go&#39;)

training_set = [
    (&#39;we are a bank specialized in dealing with IT companies&#39;, classes[3]),
    (&#39;we sell our product at a fixed cost of 10 pounds&#39;, classes[0]),
    (&#39;the cost per click is 0.01 dollars but if you get more than 10000 impression the cost will be 0.12&#39;, classes[1]),
    (&#39;we take a 1% commission on all sales, overseas sales have an additional charge of 12%&#39;, classes[2]),
    (&#39;we charge a 1% on top of your final price.&#39;, classes[2]),
    (&#39;we sell our product at 5 pounds, excluding with the variant A which costs an extra of 55 pounds&#39;, classes[0]),
    (&#39;we sell our product at 6 pounds, excluding with the variant B which costs 45 pounds&#39;, classes[0]),
    (&#39;our commission is normally between 1% and 2%&#39;, classes[2]),
    (&#39;impressions on the homepage on sundays are worth 0.01 pounds&#39;, classes[1]),
    (&#39;we will show impressions only to users that correspond to certain criteria.&#39;, classes[1]),
    (&#39;we manage an hedge fund and we take care of placing investments on behalf of our clients&#39;, classes[3]),
    (&#39;we bill only for the amount of api you use. 0.10 per 1000 calls&#39;, classes[4]),
    (&#39;running a virtual machine will cost you 0.12 pounds per hour&#39;, classes[4]),
    (&#39;we invest in major hedge funds&#39;, classes[3]),
    (&#39;we are an international bank, based in all countries of europe&#39;, classes[3]),
]

test_text = &amp;quot;we are a hedge fund collaborating with many banks in europe&amp;quot;
test_text2 = &amp;quot;we charge a fixed fee on top of our client&#39;s sales&amp;quot;

if __name__ == &#39;__main__&#39;:
    classifier = Classifier(training_set)
    classifier.run()
    print &amp;quot;%s -&amp;gt; classified as: %s&amp;quot; % (test_text, classifier.classify(test_text))
    print &amp;quot;%s -&amp;gt; classified as: %s&amp;quot; % (test_text2, classifier.classify(test_text2))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can run this code and classify entities based on their preferred sales target. Some of the above lines are commented, uncomment them if you think it gives you a better representation of the example. Just add a further 1000 good examples and then it should start to make accurate decisions&amp;hellip; enjoy!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Logging</title>
      <link>http://flagzeta.org/blog/logging/</link>
      <pubDate>Fri, 08 Apr 2011 01:00:10 +0000</pubDate>
      
      <guid>http://flagzeta.org/blog/logging/</guid>
      <description>&lt;p&gt;I work on e-commerce platforms, logging is a critical component in this area. Learn how to do it correctly will allow you and everybody else to save time when problems happen. Sometimes it is not just about saving time, but also being able to give correct answers to customers when things go wrong. When interacting with third parties, logging is even more important, because it allows you to understand where the problem lies, in your code or in the external service.&lt;/p&gt;

&lt;p&gt;First basic rule is use a logging framework. There are plenty out there, for any language. Modern logging framework are quite complex, they allow you to use different methods of logging (file, database, etc..), log rotation, log levels, email triggering, etc&amp;hellip; you should refrain from inventing your own, usually these are pretty extensible anyway. All of the frameworks i have seen have usually the same sort of interface, level based&amp;hellip; and they are quite easy to use.&lt;/p&gt;

&lt;p&gt;Once you set up your framework, there is a lot to say about how you log things. Do not pretend that people already know your code inside out when they read your application logs. In fact, high are the chances that you do not remember how you coded the functionality if you are looking at logs of old code. Do not give implementation insights in the log files, talk about what the code is doing using the language of the application domain. If you are building a billing application, use billing specific vocabularies. If it is a payment gateway, try to reuse the 3rd party terminology. If you need to, you can quickly lookup the term you used in their documentation and discover what that means. That is something you cannot do if you made that up.&lt;/p&gt;

&lt;p&gt;After having chosen how to log things, consider every log call and give it a degree of severity. From an ignorant point of view, you know you should start worrying when you see lots of &amp;ldquo;Critical&amp;rdquo; inside the log files, even if you do not know the application domain. If you see one, it is probably a good idea to take a look at the code or at least notify the author.&lt;/p&gt;

&lt;p&gt;For critical errors or errors that need to be fixed, useful is to include stats (cpu, memory usage, etc) and location of the code that was being executed. If you had an exception triggered, log the exception stack trace.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>