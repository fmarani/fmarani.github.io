<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Blog | Federico Marani</title>
    <link>/blog/</link>
    <description>Recent content in Blogs on Blog | Federico Marani</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</copyright>
    <lastBuildDate>Thu, 15 Sep 2016 12:23:07 +0200</lastBuildDate>
    <atom:link href="/blog/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Do not use HTTPS and basic auth</title>
      <link>/blog/do-not-use-https-and-basic-auth/</link>
      <pubDate>Thu, 15 Sep 2016 12:23:07 +0200</pubDate>
      
      <guid>/blog/do-not-use-https-and-basic-auth/</guid>
      <description>&lt;p&gt;Security is a difficult topic, the discipline is very &amp;ldquo;deep&amp;rdquo;, therefore it is easy to make mistakes if you do not dig deep enough. Unfortunately many people misjudge perceived security with real security, on the basis of &amp;ldquo;it makes sense&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;HTTPS is a wonderful protocol, it gives you encryption therefore you cannot see what is going on at L7 (urls, http headers, http verbs, app data). This level of obscurity may be enough for simple apps. The fact that your app knows your private REST api structure is already a (weak) client authentication proof. &lt;strong&gt;Basic auth does not give you any extra security on top of that&lt;/strong&gt;, because if you can read the REST urls (with a MITM attack), you can read the password hash, and if you know the password hash, you can send requests as the original client. You can reuse the hash to reauthenticate yourself.&lt;/p&gt;

&lt;p&gt;In most cases, it is enough for your app to &lt;strong&gt;verify the server SSL certificate&lt;/strong&gt;. When you do that, it gets almost impossible to hack the connection and read data from it.&lt;/p&gt;

&lt;p&gt;If you want some extra security, you could:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Introduce API keys and passwords for app users. It is going to be application dependent, so up to you to build the verification for this. &lt;strong&gt;This is not a strong authentication model&lt;/strong&gt;, but because they are User-specific, if they are compromised, these credentials can be easily revoked and reset. Many SaaS use this method, because it is very easy to deploy at scale and to understand.&lt;/li&gt;
&lt;li&gt;Use HTTP Digest auth, or something with a randomized pre-shared nonce. Differently from all the previous ones, this is not vulnerable to replay attacks, therefore it is more secure.&lt;/li&gt;
&lt;li&gt;Have some mechanism to sign HTTP requests. This is not an authentication mechanism, but it protects against data tampering. If an hacker sees that you are protected against replay attacks, the next logical step is modify existing requests on the fly. If the request is signed, this will be much harder.&lt;/li&gt;
&lt;li&gt;Use HTTPS Client certificates, that the server authenticates. This is a bit harder to put in place and theoretically offers maximum protection.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Security is not an immutable thing in time. Things that were secure 5 years ago might not be considered secure today. Security is also not an absolute, there is no 100% secure. Your only option is to defend yourself down every level of your stack, hoping that whoever wants to hack you either loses interest in you or runs out of hacking ideas.&lt;/p&gt;

&lt;p&gt;All the above options can be used in combination, because they offer different benefits. Evaluating the right combination is up to you. This is a run-through all possible options, focusing only on security at transport level. There is a lot more to say about security of the device. If an attacker is able to get access to the application code, the surface of attack becomes bigger.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rebuilding my site with Hugo (and IPFS)</title>
      <link>/blog/hugo-and-ipfs/</link>
      <pubDate>Wed, 14 Sep 2016 17:04:43 +0200</pubDate>
      
      <guid>/blog/hugo-and-ipfs/</guid>
      <description>

&lt;h2 id=&#34;hugo&#34;&gt;Hugo&lt;/h2&gt;

&lt;p&gt;This site was built using Django and, given the very simple content model that it had (and almost never changed), I decided to rebuild it in &lt;a href=&#34;http://gohugo.io&#34;&gt;Hugo&lt;/a&gt;. I also was getting fed up of mantaining yet another dynamic site, along with runtime and databases. I picked Hugo because it is written in Go, therefore very easy to get going (one file to install) and it works on any platform.&lt;/p&gt;

&lt;p&gt;There are 2 steps for this conversion:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Convert the Django template files to Go template files. Converting any simple logic into either Go template logic or JS if needs to be evaluated every time.&lt;/li&gt;
&lt;li&gt;Cycle over all posts, convert them into files. Write them in a directory (under content) that uses names from the current URLs.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There might be more steps in your setup. In mine, I had to transfer some media files, and define another archetype for normal pages. You can see that at &lt;a href=&#34;https://github.com/fmarani/blog&#34;&gt;my repo&lt;/a&gt;. If you can deal with the Go template language, Hugo is a very nice software.&lt;/p&gt;

&lt;h2 id=&#34;github-pages&#34;&gt;Github pages&lt;/h2&gt;

&lt;p&gt;At this point, the architecture was already much simpler. But I wanted to get rid of the hassle of hosting it. Github pages is free and works pretty well (S3 could also be an option). While I was in the process of following their tutorials, the only slightly obscure step was that I needed two repositories, one for the original Hugo site, and another one for the compiled version. Forget all the tutorials that tell you to create a &lt;code&gt;gh-pages&lt;/code&gt; branch, those are for project sites, not user sites.&lt;/p&gt;

&lt;p&gt;I ended up with:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/fmarani/blog&#34;&gt;the original repo&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/fmarani/fmarani.github.io&#34;&gt;the compiled repo&lt;/a&gt;. Please note the name of the repo, that&amp;rsquo;s how Github needs it.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The only additional step after all this was to make my domain point to Github, you can find some instructions on their site for that.&lt;/p&gt;

&lt;h2 id=&#34;ipfs&#34;&gt;IPFS&lt;/h2&gt;

&lt;p&gt;Given that all the content is static, it lends itself very well to be distributed rather than just staying on a central server. I installed IPFS on a machine I had available, configured it and let it run as a daemon (quite easy, plenty of docs online). In case Github is down, you can also find this site on IPFS.&lt;/p&gt;

&lt;p&gt;You can either use the &lt;a href=&#34;http://gateway.ipfs.io/ipns/QmfEMiRfCDtPs9B1UsCLWgRWWFp7ZUwZLU2oPWMTqzPKm3/&#34;&gt;gateway&lt;/a&gt; or use the IPNS directly (QmfEMiRfCDtPs9B1UsCLWgRWWFp7ZUwZLU2oPWMTqzPKm3).&lt;/p&gt;

&lt;p&gt;The only caveat I would like to underline here is that you have to use &lt;code&gt;relativeurls = true&lt;/code&gt; in your Hugo configuration, otherwise absolute URLs will not work well with the IPFS gateway.&lt;/p&gt;

&lt;h2 id=&#34;glue-everything-together&#34;&gt;Glue everything together&lt;/h2&gt;

&lt;p&gt;I created this simple deploy script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash

# Build the project.
hugo

echo -e &amp;quot;\033[0;32mDeploying updates to Github pages...\033[0m&amp;quot;

cd public
git add -A
msg=&amp;quot;rebuilding site `date`&amp;quot;
if [ $# -eq 1 ]
  then msg=&amp;quot;$1&amp;quot;
fi
git commit -m &amp;quot;$msg&amp;quot;
git push origin master
cd ..

echo -e &amp;quot;\033[0;32mDeploying updates to IPFS...\033[0m&amp;quot;

scp -r public/ USER@HOST:repo
hash=`ssh USER@HOST ipfs add -r -q repo | tail -1`
ssh USER@HOST ipfs name publish $hash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just change the USER@HOST part with your IPFS server. If you run it locally, no need to ssh.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>YCombinator interview</title>
      <link>/blog/yc-interview/</link>
      <pubDate>Sun, 01 May 2016 20:43:48 +0000</pubDate>
      
      <guid>/blog/yc-interview/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;../attachments/ycombinator.png&#34; alt=&#34;YC&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A month ago me and my co-founder did an application for YC, and last week we were lucky enough to be selected for an interview, along with other startups. We got called all the way to California with not even a week of notice, everything paid&amp;hellip; definitely a bit more serious than a phone call.&lt;/p&gt;

&lt;p&gt;We prepared a lot during the last few days, reading what people say online, meeting previous YC companies, doing mock interviews with YC alumni. Some of that was YC specific, as we knew they are very product-focused. Little aside here: I am an engineer and I really dislike sales talk, because most sales words are so vague that are worthless at describing specifics. It was very refreshing to see that YC thought it the same way I did.&lt;/p&gt;

&lt;p&gt;The interview only lasted 10 minutes, although we got a second interview with other partners, so they had listened to our story twice. There were 3-4 groups of 3 partners doing interviews for max 10 min throughout the day, for 2 weeks, so you can imagine there were a lot of companies there, all with their online application accepted. I have spoken with a few of them, all clearly smart, although at that stage you practiced your pitch so many times that you do not want to hear yourself having to say it again :-)&lt;/p&gt;

&lt;p&gt;If there is one take-away for me is: explain the product, as opposed to sell the product. Explain it in a way that someone technical might be able to rebuild it. Describe it well in a way that sales processes, addressable markets, customer acquisition strategies and costs, LTV, who buys and who uses it, all these should all come naturally from the product description. You should have a good answer for all these, even if not perfect yet.&lt;/p&gt;

&lt;p&gt;In the same evening we received a negative answer, along with some good feedback. When things do not go as you would, it is easy to be dismissive, but really, there is a lot that could be learnt if you were in. I would definitely go back if we had the chance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Assessing quality by functionality mapping</title>
      <link>/blog/assessing-software-quality/</link>
      <pubDate>Sun, 15 Nov 2015 12:00:00 +0000</pubDate>
      
      <guid>/blog/assessing-software-quality/</guid>
      <description>&lt;p&gt;This post is about measuring how the technology supports your current product, that being a single marketable entity. If we accept that the definition of quality is having as less bugs as possible, and the more code you write the more bugs you insert, you will have to accept the fact that the more code you write the less quality you will be able to get out of it. A good software project has the right amount of code to support the features that your product strategy dictates. That I think is independent of the paradigm you adopt, monolith or microservice, functional or not, etc. This post is really about the product.&lt;/p&gt;

&lt;p&gt;A web product is made of pages and hyperlinks that bring you to other pages, all this resulting in user journeys. Your product strategy tells you what are the journeys your product needs, ux and marketing are more concerned about the how these steps are presented. Technology is influenced by both the what and the how, so we need to map both. These are some ideas on how to map out softwares based on various system architectures.&lt;/p&gt;

&lt;p&gt;Simple case (Django/RoR-type setup):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;user journey description&lt;/li&gt;
&lt;li&gt;where the html is&lt;/li&gt;
&lt;li&gt;where the backend code files are&lt;/li&gt;
&lt;li&gt;does the backend have unit tests&lt;/li&gt;
&lt;li&gt;what db tables it is using&lt;/li&gt;
&lt;li&gt;any SaaS dependency&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Single page apps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;user journey description&lt;/li&gt;
&lt;li&gt;where the html/js component is&lt;/li&gt;
&lt;li&gt;what api is using&lt;/li&gt;
&lt;li&gt;does the js component have unit tests&lt;/li&gt;
&lt;li&gt;does the js depend on other SaaS api&lt;/li&gt;
&lt;li&gt;where is the backend code to support those api&lt;/li&gt;
&lt;li&gt;does the backend component have unit tests&lt;/li&gt;
&lt;li&gt;what db tables it is using&lt;/li&gt;
&lt;li&gt;any dependency on other installed software&lt;/li&gt;
&lt;li&gt;any SaaS dependency&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Single page apps with microservices:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;user journey description&lt;/li&gt;
&lt;li&gt;where the html/js component is&lt;/li&gt;
&lt;li&gt;what api is using&lt;/li&gt;
&lt;li&gt;does the js component have unit tests&lt;/li&gt;
&lt;li&gt;does the js depend on other SaaS api&lt;/li&gt;
&lt;li&gt;where is the backend code to support those api&lt;/li&gt;
&lt;li&gt;does the backend component have unit tests&lt;/li&gt;
&lt;li&gt;what other microservices is the backend talking to&lt;/li&gt;
&lt;li&gt;what is the fallback mechanism in case the microservice is unavailable&lt;/li&gt;
&lt;li&gt;what db tables or nosql resources are these microservices using&lt;/li&gt;
&lt;li&gt;any dependency on other installed software&lt;/li&gt;
&lt;li&gt;any SaaS dependency&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The more complex your system architecture is, the more layers you will have to map, so the above list is non-exhaustive.&lt;/p&gt;

&lt;p&gt;Once you have mapped out every column presented above, you should have a good idea of how good (or entangled) your software architecture is. That is a pretty good base to assess a project&amp;rsquo;s quality. The less things you have in the list the better&amp;hellip; you will have less to justify.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Another failure, another lesson - comprotempo.it</title>
      <link>/blog/comprotempo/</link>
      <pubDate>Wed, 11 Mar 2015 12:00:00 +0000</pubDate>
      
      <guid>/blog/comprotempo/</guid>
      <description>&lt;p&gt;When it came out, I was fashinated by TaskRabbit. I thought that a generic marketplace for handymen was the perfect tool for this decade. We have lots of young unemployment in Italy and there were also middle-class people willing to pay for someone doing chores for them. The idea was simple: putting both sides in touch by agreeing location, type of task and price. The advantages for handymen were money for work, the advantages for the committant is to &amp;ldquo;buy&amp;rdquo; someone else&amp;rsquo;s time (hence the &amp;ldquo;comprotempo&amp;rdquo; name)&lt;/p&gt;

&lt;p&gt;In 2012, I set out to replicate the idea and find a good team of people to work with. I am an engineer, it is easy for us to &amp;ldquo;over-build&amp;rdquo; the product, or &amp;ldquo;over-engineer&amp;rdquo; a product feature, so I only built what I thought was the minimum product. Everybody is biased in some ways, working in a team is the only way to keep these biases in check. Once I built it, 2-3 months later, it was about spreading the word: Facebook page, Twitter account, PPC on various channels, writing content, reaching out to people, etc&amp;hellip;&lt;/p&gt;

&lt;p&gt;Turns out building a Marketplace is very hard. You have now to convince two types of customers which may have a completely different set of concerns and motivations. The money incentive is always very strong, in fact we had a lot of users registered as handymen for a very broad set of tasks. On the other hand, feeding the system with tasks has proven to be very challenging.&lt;/p&gt;

&lt;p&gt;Because we started with handling a broad set of tasks, we had a problem with audience addressability: people likely to buy services on our site were not very addressable, distinguishable from the crowd. For instance, how do you target people willing to pay for lawn mowing, and willing to trust an online service for it? Not easy. We had to do that for every type of service the site was offering, so it was hardly marketing scalable. Some of these services were by nature recurrent, most of them were not though.&lt;/p&gt;

&lt;p&gt;We were also doing a bad job at keeping people&amp;rsquo;s attention on the site. The homepage was generic on purpose, and I don&amp;rsquo;t think people were understanding the relevance to their problem. Also they were very likely to be busy people, I think we were wasting their attention timespan. Posting an ad for a task also required filling a lot of information about the task before-hand, in order to receive good money quotes for it. People don&amp;rsquo;t like long forms.&lt;/p&gt;

&lt;p&gt;Most of all, I think we had done a very bad job at building trust as a community. We had a rating system for handymen, but with no user-generated ratings it was not helpful. It is a bit of a chicken-and-egg problem sometimes, communities are not self-sustaining from the start. Building brand reputation is just really long and hard.&lt;/p&gt;

&lt;p&gt;At the end, we also had problems with the team. With internal divisions, you just cannot focus your energies anymore. I can&amp;rsquo;t stress this enough, problems with founding team are just incredibly hard to deal with. They ripple out into everything else. That was the end for us.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Monitoring servers with Munin</title>
      <link>/blog/monitoring-with-munin/</link>
      <pubDate>Fri, 16 Jan 2015 12:51:25 +0000</pubDate>
      
      <guid>/blog/monitoring-with-munin/</guid>
      <description>&lt;p&gt;I normally use Munin for server monitoring, it is very easy to install and the kind of tool with not much setup. It may not be the best tool when you have many servers, due to static graph generation. Munin 2, released recently, has a few changes in that regard, they might have improved that.&lt;/p&gt;

&lt;p&gt;Munin is mainly a resource usage graph tool, which monitors many metrics from a pool of servers (called nodes). When set up, the monitoring server connects to every node (specified in its configuration) and then asks for a list of current values for all monitored metrics, through a simple text-based protocol. Every node has a list of enabled &amp;ldquo;plugins&amp;rdquo; which will be run everytime the server connects to the node. Many plugins come installed by default with Munin, and many additional plugins are available online with an open source license (or public domain). Those plugins have default parameters in it, but much can be customized in the munin-node configuration file.&lt;/p&gt;

&lt;p&gt;These &amp;ldquo;plugins&amp;rdquo; define the metrics, then the server will render any metrics the node send, without any a priori knowledge. The security model is IP whitelisting: each node has a list of IPs allowed to ask for metrics. The server, by default, will connect to every host every 5 minutes and add all collected metrics to its database. Every hour all the html and graphs are generated and put in a folder where Nginx is able to serve these.&lt;/p&gt;

&lt;p&gt;Munin can also be configured to trigger alerts in case a metric changes state between OK, Warning and Critical. Alerts via email are easy to setup and normally enough for basic error reporting. These thresholds can be changed if needed, but the values defined by the plugins are normally good enough. Alerts for specific plugins can be disabled if needed, see &lt;a href=&#34;http://serverfault.com/questions/532319/i-have-setup-munin-how-do-i-set-up-alerts-for-specific-parameters&#34; title=&#34;here&#34;&gt;http://serverfault.com/questions/532319/i-have-setup-munin-how-do-i-set-up-alerts-for-specific-parameters&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The only daemon in the Munin architecture is munin-node, which runs on every monitored server. On the monitoring server side, everything is managed through cron. Munin is written in Perl and its core modules are quite battle-tested.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Working with caches and Memcache</title>
      <link>/blog/working-with-memcache/</link>
      <pubDate>Wed, 14 Jan 2015 11:45:46 +0000</pubDate>
      
      <guid>/blog/working-with-memcache/</guid>
      <description>&lt;p&gt;The traditional use of Memcache is to cache things like computations that take some time or result coming from external system with limited throughput. Examples of these may be database queries (sometimes long to compute), search queries coming from dedicated search services or simply feeds from other sites that don&amp;rsquo;t change often.&lt;/p&gt;

&lt;p&gt;Memcache is both a cache system and a separate component in your system architecture, so it is very important to understand the implications. There are many negatives: it is another dependency, which adds complexity to the system, it potentially contains stale data, and it is a potential point of failure. Please consider all these things before inserting a cache in your app, many annoying and hard to find bugs in software development are related to cache.&lt;/p&gt;

&lt;p&gt;On the positive side, if done right, caching means increase in performance for the website and less stress for the underlying service. Start by looking at the areas of your application which are executed very frequently and are computationally or IO heavy. For example, loading recursive structures (like category trees) from relational databases (which are flat) is an expensive operation.&lt;/p&gt;

&lt;p&gt;One common usecase is search: results depend only on a set of parameters which are repetitive. If parameters are not repetitive, they can be made to by approximating them to some common form: free text keywords can be stemmed (see NLP), geographic coordinates can be rounded, number of results can be made multiple of some base number, etc&amp;hellip; in this way we increase the cache hit rate without losing too much precision.&lt;/p&gt;

&lt;p&gt;This exercise of increasing cache hits is quite common, and it is also very common in compilers when they try to align structures in a way that they fit CPU caches. The principle is similar, you want to increase the likelihood of having fast read operations instead of slow operations.&lt;/p&gt;

&lt;p&gt;In python there are libraries to interact with memcache, as in Django. They are quite small as the protocol is very simple to understand.&lt;/p&gt;

&lt;p&gt;I am generally against systems that automatically cache all database queries: if you have a good schema, you should not need it. We used to cache all Postgresql queries but the speed gain was not noticeable so we removed it, we have a fast network, enough memory for the disk buffer and, honestly, not a great volume of complex queries.&lt;/p&gt;

&lt;p&gt;The setup of Memcache is really simple, you can start by just using the defaults after the installation. The default cache size is 64Mb and when exceeding that it will start to delete keys in least recently used order.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scaling with Ansible (Europython 2014)</title>
      <link>/blog/scaling-with-ansible/</link>
      <pubDate>Sun, 27 Jul 2014 12:00:00 +0000</pubDate>
      
      <guid>/blog/scaling-with-ansible/</guid>
      <description>&lt;p&gt;The presentation I gave at Europython 2014 in Berlin. Again it is about how TrialReach uses Ansible to manage its own infrastructure and some tips and tricks about our use of Ansible.&lt;/p&gt;

&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;38a6f0d0f48f01318f29762c3a3e275b&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;And also the video of the presentation:&lt;/p&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;//www.youtube.com/embed/ptc9E24YAcc&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Give everyone one (public) VM</title>
      <link>/blog/give-everyone-one-vm/</link>
      <pubDate>Tue, 11 Feb 2014 22:45:08 +0000</pubDate>
      
      <guid>/blog/give-everyone-one-vm/</guid>
      <description>&lt;p&gt;At TrialReach we want to be always able to deploy clean versions of our code online. This allows us to show our work more quickly internally (and externally) and get feedback from people as early as possible, without having to wait release dates. This also give us the opportunity to test more frequently our server provisioning procedures, and having the ability to push something live anytime is a really empowering feeling.&lt;/p&gt;

&lt;p&gt;We started using Ansible as our main DevOps tool, which recently we extended to also take care of DigitalOcean VM creation. DigitalOcean has very easy APIs and is well integrated with Ansible. While we use EC2 for production/staging environments, for these throw-away environments DigitalOcean offer a good price/performance trade-off.&lt;/p&gt;

&lt;p&gt;Enough said, this is a vm creation snippet:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
- name: digitalocean creation
  hosts: all
  connection: local
  vars:
    - api_key: XXXXX
    - client_id: XXXX
  tasks: 
    - name: gather user info
      command: whoami
      register: user
    - name: gather ssh pub key 
      command: cat {{ ansible_env.HOME }}/.ssh/id_rsa.pub 
      register: ssh_pub_key
    - name: generate id for this machine
      shell: hostname | cksum | awk &#39;{print $1;}&#39;
      register: machineid
    - name: copy your ssh pub key on digital ocean
      digital_ocean: &amp;gt;
          state=present
          command=ssh
          name={{ machineid.stdout }}-{{ user.stdout }}
          client_id={{ client_id }}
          api_key={{ api_key }}
          ssh_pub_key=&#39;{{ ssh_pub_key.stdout }}&#39;
      register: my_ssh
    - name: creating new digital ocean vm
      digital_ocean: &amp;gt;
          state=present
          command=droplet
          name={{ machineid.stdout }}-{{ user.stdout }}
          ssh_key_ids={{ my_ssh.ssh_key.id }}
          unique_name=yes
          client_id={{ client_id }}
          api_key={{ api_key }}
          size_id=66
          region_id=1
          image_id=1505447
          wait_timeout=500
      register: my_droplet
    - name: writing local2cloud inventory with new vm ip
      shell: cat local2cloud | sed &#39;s/CHANGE/{{ my_droplet.droplet.ip_address }}/&#39; &amp;gt; local2cloud.templated
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This script does a bunch of things, create ssh key and vm, but also makes sure people create only one VM. That is what we need for now. This snippet takes a inventory template (local2cloud) and fills it with the new droplet&amp;rsquo;s IP address, so it can used to provision the new server.&lt;/p&gt;

&lt;p&gt;To launch this script, make sure right variables are set, and make sure you have dopy installed in your virtualenv, then run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ansible-playbook -i &#39;localhost,&#39; -e ansible_python_interpreter=`which python` create_vm.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;-e makes sure uses python from your virtualenv, -i forces not to load an inventory file but use localhost directly. This last option is a bit &lt;a href=&#34;https://groups.google.com/forum/#!topic/Ansible-project/RuntoPUvqHM&#34; title=&#34;Ansible mailing-list&#34;&gt;hacky&lt;/a&gt;, hope in the future there are better ways to do this.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why my first startup failed - tabs.to</title>
      <link>/blog/why-my-first-startup-failed/</link>
      <pubDate>Sun, 26 Jan 2014 15:24:59 +0000</pubDate>
      
      <guid>/blog/why-my-first-startup-failed/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;../attachments/logo.png&#34; alt=&#34;logo&#34; title=&#34;Tabs.to&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Tabs.to was a url shortener, but with a twist, it could support multiple urls and it was displaying them with a sort of tabbed interface which you could use to switch between pages. The use case I was seeing was sending multiple links via twitter, and by doing so you would have saved space. In hindsight, it seems pretty short-sighted but that was the problem I had. This was in 2010, 4 years ago.&lt;/p&gt;

&lt;p&gt;I really liked the idea, it was simple, easy to explain, easy to pitch. The way I saw this challenge was really a growth problem, more than a revenue problem. After having grown big, we would have had a ton of data about sites, and we could have built a kickass analytics tool on it.&lt;/p&gt;

&lt;p&gt;I am pretty sure the reason why this failed was mainly overengineering, will come back to this later.&lt;/p&gt;

&lt;p&gt;I wanted the site to be accessible through Web and API. I wanted to build this in Scala and MongoDB, two technologies I did not know, for the web-serving part, and Python/RabbitMQ for the offline processing part. I wanted the site to scale to thousands of requests, and it did. It took 6 months of hard work, every day, every weekend, it took an incredible amount of energy. A good friend of mine made me a logo and a design, someone on elance made me the front-end, my other co-founder helped me define the product, do wireframes and prioritize what needed to be done.&lt;/p&gt;

&lt;p&gt;I started talking about this to people, and I also went to Hacker News in London to present this. It was a 20 minute presentation to a lot of people, it was fairly technical because I believed that the idea did not require explanation and technology was what I am passionate about. At the end I have received some good feedback, and I also had some angel investors interested in the product. People offered to mentor me, and I had interesting chats with some of them in the following weeks. We also met a lawyer for possibly patenting parts of this idea.&lt;/p&gt;

&lt;p&gt;People started to use the product, but numbers were low and fundamental problems started to appear in the product. It turns out many websites did not like to be loaded in an frame, either by giving back a white page or escaping the frame. Resolving this was going to be really tricky, to give to the user the same site, I would have had to create a browser extension and use the real browser tabs for that.&lt;/p&gt;

&lt;p&gt;Besides that, because I focused so much on technology and scaling, the energy I invested here was too high and I did not enough of what really matters in a startup, like market research, talking to other companies for integrations, offer content online myself, talking to early adopters.&lt;/p&gt;

&lt;p&gt;People have a limited amount of energy before they burn out. At some point I exhausted mine, all the energy I spent on making the perfect platform turned out to have been misplaced.&lt;/p&gt;

&lt;p&gt;I learned in the hard way from this experience, but it was really good learning. You can be really motivated at something, but motivation is not infinite, needs to be reinforced with success.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DevOps with Ansible presentation</title>
      <link>/blog/devops-with-ansible-presentation/</link>
      <pubDate>Mon, 02 Dec 2013 13:13:47 +0000</pubDate>
      
      <guid>/blog/devops-with-ansible-presentation/</guid>
      <description>&lt;p&gt;Presentation I gave at DJUGL in September 2013 about how TrialReach uses Ansible to automate server provisioning.&lt;/p&gt;

&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;cf5216a0021b013147b94e9ac2870296&#34; data-ratio=&#34;1.33333333333333&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Spatial search on multiple points in Solr</title>
      <link>/blog/spatial-search-on-multiple-points-in-solr/</link>
      <pubDate>Wed, 31 Jul 2013 12:09:43 +0000</pubDate>
      
      <guid>/blog/spatial-search-on-multiple-points-in-solr/</guid>
      <description>&lt;p&gt;At TrialReach we deal with clinical trials data, which contain a lot of spatial information. Tipically, clinical trials treat a certain set of conditions and they happen in various locations globally.
If you are a patient then searching across clinical trials becomes really spatial sensitive: you are only interested in the closest location to you.&lt;/p&gt;

&lt;p&gt;This case might apply to other events as well, but the key point is global distribution. I am not interested in any point in the globe, just the closest to me.&lt;/p&gt;

&lt;p&gt;&lt;h2&gt;Solution&lt;/h2&gt;
Solr 4 does have support for this with the new spatial field called SpatialRecursivePrefixTreeFieldType, with many caveats though.&lt;/p&gt;

&lt;p&gt;A schema could look this way:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot; ?&amp;gt;
&amp;lt;schema name=&amp;quot;example&amp;quot; version=&amp;quot;1.5&amp;quot;&amp;gt;
 &amp;lt;fields&amp;gt;
   &amp;lt;field name=&amp;quot;id&amp;quot; type=&amp;quot;string&amp;quot; indexed=&amp;quot;true&amp;quot; stored=&amp;quot;true&amp;quot; required=&amp;quot;true&amp;quot; multiValued=&amp;quot;false&amp;quot; /&amp;gt; 
   &amp;lt;field name=&amp;quot;title&amp;quot; type=&amp;quot;text_en&amp;quot; indexed=&amp;quot;true&amp;quot; stored=&amp;quot;true&amp;quot; required=&amp;quot;true&amp;quot;/&amp;gt;
   &amp;lt;field name=&amp;quot;condition&amp;quot; type=&amp;quot;text_en&amp;quot; indexed=&amp;quot;true&amp;quot; stored=&amp;quot;true&amp;quot; required=&amp;quot;true&amp;quot; multiValued=&amp;quot;true&amp;quot;/&amp;gt;
   &amp;lt;field name=&amp;quot;location&amp;quot; type=&amp;quot;location_rpt&amp;quot; indexed=&amp;quot;true&amp;quot; stored=&amp;quot;true&amp;quot; multiValued=&amp;quot;true&amp;quot;/&amp;gt;
   &amp;lt;field name=&amp;quot;_version_&amp;quot; type=&amp;quot;long&amp;quot; indexed=&amp;quot;true&amp;quot; stored=&amp;quot;true&amp;quot; /&amp;gt;
 &amp;lt;/fields&amp;gt;
 ... 
  &amp;lt;types&amp;gt;
 ...
    &amp;lt;fieldType name=&amp;quot;location_rpt&amp;quot; class=&amp;quot;solr.SpatialRecursivePrefixTreeFieldType&amp;quot;
        geo=&amp;quot;true&amp;quot; distErrPct=&amp;quot;0.025&amp;quot; maxDistErr=&amp;quot;0.000009&amp;quot; units=&amp;quot;degrees&amp;quot; /&amp;gt;
 &amp;lt;/types&amp;gt;
&amp;lt;/schema&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A sample indexer using GeoDjango and PySolr (Haystack does not support this). It should be quite easy to work out how it works, PySolr is just a very thin wrapper for doing HTTP POST requests to Apache Solr.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pysolr

solr = pysolr.Solr(&amp;quot;http://1.2.3.4:8983/solr/&amp;quot;, timeout=10)

records = models.Study.objects.all()
solr_data = []
for record in records:
    solr_dict = {
                &amp;quot;id&amp;quot;: str(record.id),
                &amp;quot;title&amp;quot;: record.title,
                &amp;quot;condition&amp;quot;: [c.name for c in record.conditions.all()],
                &amp;quot;location&amp;quot;: [&amp;quot;{1} {0}&amp;quot;.format(l.point.coords[0], l.point.coords[1]) for l in record.locations.all()],
        # &amp;quot;point&amp;quot; is a Point GeoDjango type
        # SOLR FORMAT is &amp;quot;long lat&amp;quot;, separated by a space
            }
    solr_data.append(solr_dict)
solr.add(solr_data)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For querying, we use these sort of urls:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://1.2.3.4:8983/solr/select/?sort=score+asc&amp;amp;fq=title:lupus+condition:lupus&amp;amp;q={!geofilt score=distance sfield=location pt=LAT,LONG d=KM_RADIUS}&amp;amp;fl=*,score
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;to return the distance you need to use the score, and the only thing you use in the q parameter is the geofilt (otherwise will influence the score), all other filters go in fq&lt;/li&gt;
&lt;li&gt;if you do not need the distance, loose the score parameter in geofilt (it is inefficient)&lt;/li&gt;
&lt;li&gt;distance returned is the distance between specified LAT,LONG and the closest LAT,LONG in the SpatialRecursivePrefixTreeFieldType set.&lt;/li&gt;
&lt;li&gt;score returned is in DEGREES. You have to convert it in Km or miles.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;h2&gt;Shortcomings&lt;/h2&gt;
- the only way to get the distance is through the score
- you cannot get the matched point through highlighting or any other way
- units of measure are a bit confusing&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Transifex for your Django projects</title>
      <link>/blog/transifex-for-your-django-projects/</link>
      <pubDate>Thu, 31 Jan 2013 15:39:37 +0000</pubDate>
      
      <guid>/blog/transifex-for-your-django-projects/</guid>
      <description>&lt;p&gt;I am assuming you already created a project on Transifex (in this example is TxProject), either on the hosted version or on the downloadable version, and all the users you need are on there (just one to start is enough). I am also assuming i18n is already setup and you have at least 2 languages already in your project.&lt;/p&gt;

&lt;p&gt;The aim of integrating Transifex libraries into your code is to make it really easy to push/pull translations of a project to their web front-end.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pip install transifex-client&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;First thing is to install their python client, make things much easier instead of manually uploading PO files.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;user@host:/workspace/project$ tx init&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This creates a .tx folder in your project root to store all tx configuration. You should include this in the repository.&lt;/p&gt;

&lt;p&gt;Now suppose you have multiple apps in your django project. For each of those, you should have a locale/ folder inside it with all the application PO files. You need to generate a source language PO file before linking to transifex.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;user@host:/workspace/project/apps/main$ ../../manage.py makemessages -l en&lt;/p&gt;

&lt;p&gt;user@host:/workspace/project$ tx set &amp;ndash;auto-local -r TxProject.main &amp;lsquo;apps/main/locale/&lt;lang&gt;/LC_MESSAGES/django.po&amp;rsquo; &amp;ndash;source-lang en &amp;ndash;source-file apps/main/locale/en/LC_MESSAGES/django.po -t PO&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Repeat the last command for every app you have in your project, changing the resource name (-r option) in TxProject.APPNAME. Next step is to push all your PO files to transifex.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;user@host:/workspace/project$ tx push -s -t&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;After the translations have been done on Transifex, you can pull them into your project by typing.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;user@host:/workspace/project$ tx pull&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;All very nice and easy!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Technology behind tools.seogadget.co.uk</title>
      <link>/blog/technology-behind-toolsseogadgetcouk/</link>
      <pubDate>Sun, 25 Nov 2012 17:30:21 +0000</pubDate>
      
      <guid>/blog/technology-behind-toolsseogadgetcouk/</guid>
      <description>&lt;p&gt;Scalability was one of the primary concerns when we started building the tool. Essentially, the tool gathers numbers about links you post, it is quite straightforward. To gather these numbers, our tool uses many external APIs and in a way acts as a sort of proxy between the user and many other 3rd party API providers, on top of which some internal indicators are derived. Many tools allow you to do that, but, regarding scalability, some ways are better than others. Much better actually. Gathering information for 1000 urls a day is different than doing it on 1 million, lots of challenges came in the way.
&lt;h2&gt;TECHNOLOGY&lt;/h2&gt;
Deciding on which platform to use, we ended up using the well-known combo Python-Django-Celery. It is the one i have most experience with, and the task is really I/O bound therefore it is not one of those cases in which writing everything in C makes a big difference. This combo also allows us to code things pretty quickly, testing various methods and combinations. The real complexity is in the Celery backend, which is where the data gathering takes place.
&lt;h2&gt;WORKFLOW&lt;/h2&gt;
Requests could come in through API or through the Web interface. Web interface is a better example because that is the only way now to send multiple urls at once. When URLs enter into the system, each one of those is done in parallel. For every url, there are two rounds of data gathering, the first gets part of the final results, and then a second round gets the results that are dependent on the first round of numbers.&lt;/p&gt;

&lt;p&gt;All these single rounds of API calls are done asynchronously, not sequentially. We make heavy use of Celery advanced features such as tasksets and chords to make sure we squeeze every bit of performance we can from the system.&lt;/p&gt;

&lt;p&gt;Each background task takes then care of storing these numbers in a PostgreSql database server, which they later get pulled back in the Web interface (or API results)
&lt;h2&gt;INFRASTRUCTURE&lt;/h2&gt;
Heroku has allowed us to build something quickly, although we had to switch to an hybrid EC2 - Heroku, mainly because of heavy use of RabbitMQ. The advantage of Heroku is that you can scale the number of instances pretty quickly if there is a lot of traffic. We distribute the background tasks using RabbitMQ which has gone through some configuration changes. Some of the more interesting tweaks have gone into the configuration of Celery, especially on setting expiration limits for every single external API call to 3rd party systems. We do not want 3rd party APIs failure to bring down our service. All this has been wrapped up in a quite minimal interface, using Twitter Bootstrap as a CSS framework. Very easy to use.
&lt;h2&gt;IDEAS FOR THE FUTURE&lt;/h2&gt;
There has been some thought about improving the &amp;ldquo;spam&amp;rdquo; flag with something which can learn and adapt to new types of spam. What features to take into account when deciding about spammy links is also under review. There is also a lot of enhancements we can do on the APIs, such as different tiers, perhaps a tier with a different priority (e.g. reduced response time) or different limits which will be a paid option. There is also always room for speed improvements such as bulk queries, result caching, etc&amp;hellip; what is the feature you would like to see in this tool?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Basic things to do and not in Django</title>
      <link>/blog/basic-things-to-do-and-not-in-django/</link>
      <pubDate>Sun, 23 Sep 2012 19:33:30 +0000</pubDate>
      
      <guid>/blog/basic-things-to-do-and-not-in-django/</guid>
      <description>&lt;p&gt;Make software dependent on absolute paths: One of the projects i was working on had all module imports including the project main folder name. That in turn made impossible to have the same project installed 2 times in the same folder with 2 distinct names. (e.g. 2 versions of the staging site). Sometime is even worse than this, when you have a full path in the code. These should always be in configuration files which are changeable at deployment.&lt;/p&gt;

&lt;p&gt;Massive views.py file: It is probably time to split the project in separate applications. Also try to remove the code which is not directly view related and separate it in different files. Django has a very good form validation framework, if you use it your views.py file will shrink considerably.&lt;/p&gt;

&lt;p&gt;Please no grammatical mistakes: This is very bad and means you did not care about the project enough, plus not everyone is using IDEs so you should make the effort to write function names properly.&lt;/p&gt;

&lt;p&gt;Don&amp;rsquo;t throw everything in the database: I hate when applications are dependent on huge models, models should be lean and you should be able to recreate them easily enough with fixtures. For the sake of forensics, you really should make good use of logging and perhaps keep a backup of the imported feeds. Not all data needs to be in the database, only the one your project uses.&lt;/p&gt;

&lt;p&gt;Make all a varchar: Relational databases are strongly typed and this is the reason why they can do all the things they do&amp;hellip; if you want more flexibility, use mongodb.&lt;/p&gt;

&lt;p&gt;Always have automated deployments: Even if all you do is rsync to a server, you should have that command in a bash or fabric script.&lt;/p&gt;

&lt;p&gt;Not reinvent Django features: Unless there is no way to solve your problem with existing tools&amp;hellip; usually Django modules are pretty extensible and rock-solid.&lt;/p&gt;

&lt;p&gt;Use django-extensions and Django debug toolbar: It&amp;rsquo;s like going camping and not having the swiss army knife. My favourite parts are the graph models extensions which makes you an image representing all models and connections between them, and the runserver_plus which uses the Werkzeug debugger to run your code&amp;hellip; very handy the debugger. Regarding the debug toolbar, makes it really easy to diagnose what&amp;rsquo;s gone wrong when rendering a page: are all variables included in the template, some bad value coming back from the db, etc&amp;hellip;&lt;/p&gt;

&lt;p&gt;Include everything needed for the project in the repository: No files laying around the server should exist unless they are checked in the repo, this includes apache conf files&amp;hellip; unless you have a separate repository for them.&lt;/p&gt;

&lt;p&gt;Always use virtualenv: Really, projects without virtualenv are a thing of the past, and using it is trivial. Another thing to do is always have a requirements.txt in the repository, so you can recreate the virtualenv easily.&lt;/p&gt;

&lt;p&gt;Keep the project clean: Which means remove old features when they are not required anymore, just like you would clean your room from time to time. Keep in mind that all code on the site need to be maintained, and if it&amp;rsquo;s not worth maintaining it anymore, it&amp;rsquo;s time to get rid of it.&lt;/p&gt;

&lt;p&gt;Always use RequestContext and STATIC_URL for rendering templates: In that case you do not hard-code links to your static media. It&amp;rsquo;s one of those things easy to do and will make your life easier when you will have a separate static server or serve files through a CDN.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>